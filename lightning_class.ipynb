{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "from os import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint, LearningRateFinder\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'genre', 'summary', 'input_ids', 'att_mask', 'label',\n",
       "       'mapped_inputs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_parquet('./data/books.par')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(data_utils.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.input_ids=torch.tensor(df.mapped_inputs, dtype=torch.int32)\n",
    "        self.att_masks=torch.tensor(df.att_mask, dtype=torch.float32)\n",
    "        self.y=torch.tensor(df.label.values, dtype=torch.uint8)\n",
    "        self.no_classes=df.label.nunique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indexes):\n",
    "        return self.input_ids[indexes], self.att_masks[indexes], self.y[indexes]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[np.random.permutation(df.shape[0])].reset_index(drop=True)\n",
    "split=int(df.shape[0]*0.9)\n",
    "\n",
    "train_df=df.iloc[:split]\n",
    "val_df=df.iloc[split:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02371983, 0.03087905, 0.04174903, 0.03486904, 0.03410268,\n",
       "       0.02046161, 0.21651239, 0.18620066, 0.20688962, 0.2046161 ])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,label_weights=np.unique(train_df.label, return_counts=True)\n",
    "label_weights=1/label_weights\n",
    "label_weights=label_weights/np.sum(label_weights)\n",
    "label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=BookDataset(train_df)\n",
    "val_data=BookDataset(val_df)\n",
    "\n",
    "train_dataloader=data_utils.DataLoader(train_data, batch_size=32, num_workers=cpu_count(),\n",
    "                                       shuffle=True, drop_last=True)\n",
    "val_dataloader=data_utils.DataLoader(val_data, batch_size=32, num_workers=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pl module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookGenreClassifier(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-2, loss=nn.CrossEntropyLoss(), l2=1e-5, lr_dc_step=3, lr_dc=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model','loss'])\n",
    "        self.save_hyperparameters({'name':model.name})\n",
    "        self.lr=lr\n",
    "        self.loss=loss\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[:-1]\n",
    "        y = batch[-1]\n",
    "        logits=self(x)\n",
    "        loss=self.loss(logits, y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, batch, mode=None):\n",
    "        x = batch[:-1]\n",
    "        y = batch[-1]\n",
    "        logits=self(x)\n",
    "\n",
    "        loss=self.loss(logits, y)\n",
    "\n",
    "        preds=torch.argmax(logits, axis=1)\n",
    "        acc=torch.sum(preds==y)/y.shape[0]\n",
    "        # TODO add more metrics\n",
    "\n",
    "        if mode:\n",
    "            self.log(mode+'_loss', loss,  prog_bar=True)\n",
    "            self.log(mode+'_acc', 100*acc,  prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        return self.evaluate(batch, \"val\")\n",
    "    def test_step(self, batch, *args, **kwargs):\n",
    "        return self.evaluate(batch, \"test\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.hparams.l2\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            patience=self.hparams.lr_dc_step,\n",
    "            factor=self.hparams.lr_dc,\n",
    "            cooldown=1,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"strict\": False,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"name\": \"scheduler_lr\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module): \n",
    "    def __init__(self, in_dim=7031, hid_dim=128, out_dim=10):\n",
    "        # dummy model as an example, just one hidden layer straight from list of tokens\n",
    "        super().__init__()\n",
    "        self.name='DummyModel'\n",
    "        self.l1=nn.Linear(in_dim, hid_dim)\n",
    "        self.nonlinear=nn.Tanh()\n",
    "        self.l2=nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_ids, att_mask = x\n",
    "        x=in_ids*att_mask\n",
    "        x=self.l1(x)\n",
    "        x=self.nonlinear(x)\n",
    "        return self.l2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_model=BookGenreClassifier(DummyModel(), loss=nn.CrossEntropyLoss(weight=torch.tensor(label_weights, dtype=torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | DummyModel       | 901 K \n",
      "-------------------------------------------\n",
      "901 K     Trainable params\n",
      "0         Non-trainable params\n",
      "901 K     Total params\n",
      "3.606     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 130/130 [00:02<00:00, 61.23it/s, v_num=2, train_loss=2.300, val_loss=2.310, val_acc=12.90] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 130/130 [00:02<00:00, 60.53it/s, v_num=2, train_loss=2.300, val_loss=2.310, val_acc=12.90]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(dm_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4916, 32523, 16538,  ..., 36332, 36332, 36332], dtype=torch.int32),\n",
       " tensor([1., 1., 1.,  ..., 0., 0., 0.]),\n",
       " tensor(4, dtype=torch.uint8))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36333"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_set=set()\n",
    "for tokens in df.mapped_inputs.values:\n",
    "    tokens_set=tokens_set.union(set(tokens))\n",
    "vocab_size=len(tokens_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.name='SimpleRNN'\n",
    "        # last token states 'end of string' and is repeated multiple time at the end of an input\n",
    "        # therefore set its embedding to 0 with padding_idx=-1\n",
    "        self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1) \n",
    "        self.model=nn.RNN(input_size=hid_dim, hidden_size=hid_dim, batch_first=True)\n",
    "        self.out_layer=nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, _=x\n",
    "        rnn_input=self.emb(inputs)\n",
    "        _, hn=self.model(rnn_input)\n",
    "        #use last hidden state as sequence representation\n",
    "        hn=hn.squeeze()\n",
    "        return self.out_layer(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model=BookGenreClassifier(SimpleRNN(128, 10, vocab_size=vocab_size), \n",
    "                             loss=nn.CrossEntropyLoss(),#weight=torch.tensor(label_weights, dtype=torch.float32),\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=30,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "         #   LearningRateFinder(min_lr=1e-4, num_training_steps=1000)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkpuchalskixiv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240615_153110-5j694jnr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/5j694jnr' target=\"_blank\">wobbly-fog-1</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/5j694jnr' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/5j694jnr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | SimpleRNN        | 4.7 M \n",
      "-------------------------------------------\n",
      "4.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 M     Total params\n",
      "18.740    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 130/130 [00:05<00:00, 23.92it/s, v_num=4jnr, train_loss=1.900, val_loss=2.050, val_acc=24.20]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(rnn_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>scheduler_lr</td><td>██████▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▁▇▅▆▅▃▂▅▁▆▅▄▄█▂▄▄▃▃▄▃▃▅█▄▄▄▅▄▁▃▆▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>val_acc</td><td>▃▆█▃▁███▆████</td></tr><tr><td>val_loss</td><td>▇▅▆█▇▇▂▁▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>scheduler_lr</td><td>0.001</td></tr><tr><td>train_loss</td><td>1.85558</td></tr><tr><td>trainer/global_step</td><td>1689</td></tr><tr><td>val_acc</td><td>24.24893</td></tr><tr><td>val_loss</td><td>2.04645</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-fog-1</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/5j694jnr' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/5j694jnr</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240615_153110-5j694jnr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_stuff",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
