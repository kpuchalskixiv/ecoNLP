{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "from os import cpu_count\n",
    "from types import NoneType\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint, LearningRateFinder\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'genre', 'summary', 'input_ids', 'att_mask', 'label',\n",
       "       'mapped_inputs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_parquet('./data/books.par')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./data/tweets.csv', names=['index','genre','summary']).drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([149985, 835])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, AutoTokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token \n",
    "tokenized_summary=gpt2_tokenizer(df.summary.map(lambda x: x.strip().lower()).tolist(),\n",
    "                    return_tensors='pt',\n",
    "                    padding=True)\n",
    "tokenized_summary['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38690"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_summary['input_ids'][0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kacper/ecoNLP/lightning_class.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokens_dict\u001b[39m=\u001b[39m{}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tokenized_summary[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49munique()):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     tokens_dict[t\u001b[39m.\u001b[39mitem()]\u001b[39m=\u001b[39mi\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/_tensor.py:913\u001b[0m, in \u001b[0;36mTensor.unique\u001b[0;34m(self, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    904\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    905\u001b[0m         Tensor\u001b[39m.\u001b[39munique,\n\u001b[1;32m    906\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    911\u001b[0m         dim\u001b[39m=\u001b[39mdim,\n\u001b[1;32m    912\u001b[0m     )\n\u001b[0;32m--> 913\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49munique(\n\u001b[1;32m    914\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    915\u001b[0m     \u001b[39msorted\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39msorted\u001b[39;49m,\n\u001b[1;32m    916\u001b[0m     return_inverse\u001b[39m=\u001b[39;49mreturn_inverse,\n\u001b[1;32m    917\u001b[0m     return_counts\u001b[39m=\u001b[39;49mreturn_counts,\n\u001b[1;32m    918\u001b[0m     dim\u001b[39m=\u001b[39;49mdim,\n\u001b[1;32m    919\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/functional.py:991\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m    989\u001b[0m     \u001b[39mreturn\u001b[39;00m _unique_impl(\u001b[39minput\u001b[39m, \u001b[39msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 991\u001b[0m output, _, _ \u001b[39m=\u001b[39m _unique_impl(\u001b[39minput\u001b[39;49m, \u001b[39msorted\u001b[39;49m, return_inverse, return_counts, dim)\n\u001b[1;32m    992\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/functional.py:905\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    897\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39munique_dim(\n\u001b[1;32m    898\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m    899\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m         return_counts\u001b[39m=\u001b[39mreturn_counts,\n\u001b[1;32m    903\u001b[0m     )\n\u001b[1;32m    904\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 905\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_unique2(\n\u001b[1;32m    906\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    907\u001b[0m         \u001b[39msorted\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39msorted\u001b[39;49m,\n\u001b[1;32m    908\u001b[0m         return_inverse\u001b[39m=\u001b[39;49mreturn_inverse,\n\u001b[1;32m    909\u001b[0m         return_counts\u001b[39m=\u001b[39;49mreturn_counts,\n\u001b[1;32m    910\u001b[0m     )\n\u001b[1;32m    911\u001b[0m \u001b[39mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens_dict={}\n",
    "for i, t in enumerate(tokenized_summary['input_ids'].unique()):\n",
    "    tokens_dict[t.item()]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_summary['input_ids']=tokenized_summary['input_ids'].apply_(lambda x: tokens_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21301"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_summary['input_ids'][0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([149985])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_summary['attention_mask'].sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(data_utils.Dataset):\n",
    "    def __init__(self, tokenized_summary, labels, idxs):\n",
    "        self.input_ids=tokenized_summary['input_ids'][idxs]\n",
    "        self.lengths=tokenized_summary['attention_mask'].sum(axis=1)[idxs]\n",
    "        self.y=torch.tensor(labels, dtype=torch.uint8)[idxs]\n",
    "        self.no_classes=2\n",
    "        self.max_len=self.input_ids.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indexes):\n",
    "        return self.input_ids[indexes], self.lengths[indexes], self.y[indexes]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs=torch.rand(df.shape[0])>0.8\n",
    "val_idxs=~train_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=TweetDataset(tokenized_summary, df.genre.values, train_idxs)\n",
    "val_data=TweetDataset(tokenized_summary, df.genre.values, val_idxs)\n",
    "\n",
    "train_dataloader=data_utils.DataLoader(train_data, batch_size=32, num_workers=cpu_count(),\n",
    "                                       shuffle=True, drop_last=True)\n",
    "val_dataloader=data_utils.DataLoader(val_data, batch_size=32, num_workers=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(data_utils.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.input_ids=torch.tensor(df.mapped_inputs, dtype=torch.int32)\n",
    "        self.lengths=torch.tensor(df.att_mask.map(sum), dtype=torch.int32)\n",
    "      #  self.att_masks=torch.tensor(df.att_mask, dtype=torch.float32)\n",
    "        self.y=torch.tensor(df.label.values, dtype=torch.uint8)\n",
    "        self.no_classes=df.label.nunique()\n",
    "        self.max_len=self.input_ids.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indexes):\n",
    "       # batch_max_len= torch.max(self.lengths[indexes])\n",
    "        return self.input_ids[indexes], self.lengths[indexes], self.y[indexes]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSampler(data_utils.Sampler):\n",
    "    def __init__(self, dataset, batch_size, shuffle=False, drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.length\n",
    "\n",
    "    def __iter__(self):\n",
    "        order = np.arange(len(self))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(order)\n",
    "        if len(self) % self.batch_size:\n",
    "            for i in range(0, len(self) - self.batch_size, self.batch_size):\n",
    "                yield order[i : i + self.batch_size]\n",
    "            if not self.drop_last:\n",
    "                yield order[-(len(self) % self.batch_size) :]\n",
    "        else:\n",
    "            for i in range(0, len(self), self.batch_size):\n",
    "                yield order[i : i + self.batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedDataset(data_utils.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.input_ids=torch.tensor(df.mapped_inputs, dtype=torch.int32)\n",
    "        self.lengths=torch.tensor(df.att_mask.map(sum), dtype=torch.int32)\n",
    "        self.y=torch.tensor(df.label.values, dtype=torch.uint8)\n",
    "        self.no_classes=df.label.nunique()\n",
    "        self.max_len=self.input_ids.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indexes):\n",
    "       \n",
    "        return pack_padded_sequence(self.input_ids[indexes].view(-1, self.max_len), \n",
    "                                    self.lengths[indexes].view(-1), \n",
    "                                    batch_first=True, enforce_sorted=False), self.y[indexes]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_elementwise_apply(fn, packed_sequence):\n",
    "    \"\"\"applies a pointwise function fn to each element in packed_sequence\"\"\"\n",
    "    return torch.nn.utils.rnn.PackedSequence(fn(packed_sequence.data), packed_sequence.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pack_padded_sequence(emb(b[0]), b[1], batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_elementwise_apply(emb, val_data[:32][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[np.random.permutation(df.shape[0])].reset_index(drop=True)\n",
    "split=int(df.shape[0]*0.9)\n",
    "\n",
    "train_df=df.iloc[:split]\n",
    "val_df=df.iloc[split:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02385822, 0.03267182, 0.04253915, 0.03501842, 0.03476234,\n",
       "       0.02115128, 0.20895605, 0.19207071, 0.20228724, 0.20668478])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,label_weights=np.unique(train_df.label, return_counts=True)\n",
    "label_weights=1/label_weights\n",
    "label_weights=label_weights/np.sum(label_weights)\n",
    "label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164841/4220535775.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  self.input_ids=torch.tensor(df.mapped_inputs, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "train_data=BookDataset(train_df)\n",
    "val_data=BookDataset(val_df)\n",
    "\n",
    "train_dataloader=data_utils.DataLoader(train_data, batch_size=32, num_workers=cpu_count(),\n",
    "                                       shuffle=True, drop_last=True)\n",
    "val_dataloader=data_utils.DataLoader(val_data, batch_size=32, num_workers=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pl module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookGenreClassifier(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-2, loss=nn.CrossEntropyLoss(), l2=1e-5, lr_dc_step=3, lr_dc=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model','loss'])\n",
    "        if isinstance(loss.weight, NoneType):\n",
    "            weighted_loss=False\n",
    "        else: weighted_loss=True\n",
    "        self.save_hyperparameters({'name':model.name, \n",
    "                                   'dropout_p':model.dropout_p,\n",
    "                                   'w2v_init':model.w2v_init,\n",
    "                                   'num_layers':model.num_layers,\n",
    "                                   'nonlinearity':model.nonlinearity,\n",
    "                                   'bidirectional':model.bidirectional,\n",
    "                                   'weighted_loss':weighted_loss})\n",
    "        self.lr=lr\n",
    "        self.loss=loss\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[:-1]\n",
    "        y = batch[-1]\n",
    "        logits=self(x)\n",
    "        loss=self.loss(logits, y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, batch, mode=None):\n",
    "        x = batch[:-1]\n",
    "        y = batch[-1]\n",
    "        logits=self(x)\n",
    "\n",
    "        loss=self.loss(logits, y)\n",
    "\n",
    "        preds=torch.argmax(logits, axis=1)\n",
    "        acc=torch.sum(preds==y)/y.shape[0]\n",
    "        # TODO add more metrics\n",
    "\n",
    "        if mode:\n",
    "            self.log(mode+'_loss', loss,  prog_bar=True)\n",
    "            self.log(mode+'_acc', 100*acc,  prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        return self.evaluate(batch, \"val\")\n",
    "    def test_step(self, batch, *args, **kwargs):\n",
    "        return self.evaluate(batch, \"test\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.hparams.l2\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            patience=self.hparams.lr_dc_step,\n",
    "            factor=self.hparams.lr_dc,\n",
    "            cooldown=1,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_acc\",\n",
    "                \"strict\": False,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"name\": \"scheduler_lr\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module): \n",
    "    def __init__(self, in_dim=7031, hid_dim=128, out_dim=10):\n",
    "        # dummy model as an example, just one hidden layer straight from list of tokens\n",
    "        super().__init__()\n",
    "        self.name='DummyModel'\n",
    "        self.dropout_p=0\n",
    "        self.l1=nn.Linear(in_dim, hid_dim)\n",
    "        self.nonlinear=nn.Tanh()\n",
    "        self.l2=nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_ids, att_mask = x\n",
    "        x=in_ids*att_mask\n",
    "        x=self.l1(x)\n",
    "        x=self.nonlinear(x)\n",
    "        return self.l2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_model=BookGenreClassifier(DummyModel(), loss=nn.CrossEntropyLoss(weight=torch.tensor(label_weights, dtype=torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/szymon/pythonvenvs/rocmwork/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0      | train\n",
      "1 | model | DummyModel       | 901 K  | train\n",
      "---------------------------------------------------\n",
      "901 K     Trainable params\n",
      "0         Non-trainable params\n",
      "901 K     Total params\n",
      "3.606     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a7ad7fdae44ddbaf25d5bd4179b128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                   | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(dm_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  150,   333,   683,  ..., 26305, 26305, 26305], dtype=torch.int32),\n",
       " tensor(496, dtype=torch.int32),\n",
       " tensor(0, dtype=torch.uint8))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kacper/ecoNLP/lightning_class.ipynb Cell 38\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# for tweets\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vocab_size\u001b[39m=\u001b[39mtokenized_summary[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmax()\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bookwords \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#for s in df.summary:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#    bookwords.append(list(tokenize(s, lowercase=True)))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_summary' is not defined"
     ]
    }
   ],
   "source": [
    "# for tweets\n",
    "vocab_size=tokenized_summary['input_ids'].max()+1\n",
    "bookwords = []\n",
    "#for s in df.summary:\n",
    "#    bookwords.append(list(tokenize(s, lowercase=True)))\n",
    "for s, a in zip(tokenized_summary['input_ids'], tokenized_summary['attention_mask']):\n",
    "    bookwords.append([str(i.item()) for i in s[a.to(bool)]]+[str(s[-1].item())])\n",
    "print(bookwords[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26306"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_set=set()\n",
    "for tokens in df.mapped_inputs.values:\n",
    "    tokens_set=tokens_set.union(set(tokens))\n",
    "vocab_size=len(tokens_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['37', '1722', '165', '17871', '2821', '203', '151', '573', '21129', '1574', '151', '1799', '660', '1291', '375', '203', '204', '469', '8020', '221', '1468', '179', '9073', '151', '348', '173', '196', '14782', '13', '530', '2898', '236', '146', '313', '6585', '5114', '14', '1617', '834', '179', '13734', '293', '1075', '13772', '58', '13', '151', '1259', '4723', '477', '9089', '203', '7329', '469', '8020', '179', '351', '264', '8048', '151', '11527', '211', '18525', '17871', '2821', '1599', '539', '13', '469', '8020', '414', '447', '5227', '173', '3543', '634', '175', '151', '2047', '1490', '173', '333', '1038', '477', '222', '5644', '173', '3077', '145', '2723', '221', '9840', '7690', '13', '4936', '11', '151', '3543', '1775', '3468', '13738', '232', '1404', '236', '151', '590', '264', '12465', '173', '151', '3857', '4252', '175', '151', '1807', '13', '9089', '203', '23667', '1264', '287', '146', '1341', '3274', '232', '3276', '25614', '11', '1592', '236', '151', '5558', '174', '13617', '11', '817', '469', '8020', '3070', '176', '333', '3213', '13', '477', '151', '936', '25603', '1531', '539', '287', '151', '16302', '1217', '151', '1410', '5512', '4457', '7416', '173', '8237', '863', '11', '469', '8020', '203', '1037', '2404', '13', '3367', '11', '146', '21052', '12488', '151', '16173', '1085', '536', '172', '11741', '19081', '221', '9840', '20772', '3085', '539', '13', '236', '2137', '236', '469', '8020', '7200', '223', '11', '333', '849', '203', '5655', '232', '146', '10498', '1895', '7043', '13', '469', '8020', '637', '336', '151', '1895', '849', '11', '287', '407', '11741', '19081', '6550', '11493', '336', '868', '333', '9840', '11', '388', '211', '222', '329', '4573', '466', '1324', '13', '279', '734', '562', '11', '146', '20764', '2211', '3274', '1217', '151', '24087', '5197', '788', '469', '8020', '13', '204', '2553', '11', '469', '8020', '238', '4063', '287', '151', '1212', '175', '469', '274', '8', '203', '4304', '173', '3409', '1194', '162', '235', '11', '151', '573', '11016', '11', '179', '173', '7739', '3073', '366', '240', '207', '13', '377', '5329', '5168', '466', '682', '151', '1376', '175', '13307', '179', '513', '151', '3857', '4252', '11', '662', '351', '264', '1324', '12738', '287', '11741', '19081', '221', '7898', '195', '3274', '11', '151', '298', '1203', '13', '151', '2047', '16728', '204', '151', '24087', '203', '2260', '26', '3492', '3409', '1194', '162', '235', '7154', '170', '5135', '1807', '25108', '11', '1310', '13', '12703', '267', '3919', '11', '173', '1077', '146', '3483', '12031', '173', '6028', '176', '151', '6817', '20521', '13', '12703', '267', '3919', '2973', '211', '469', '8020', '203', '5242', '1029', '211', '24870', '232', '333', '4328', '11', '179', '3920', '3409', '1194', '162', '235', '173', '3008', '539', '25928', '13', '236', '146', '780', '8941', '11', '469', '8020', '2091', '466', '151', '1410', '5512', '221', '936', '25603', '11', '407', '7270', '12703', '267', '3919', '1908', '211', '351', '1073', '508', '4476', '175', '469', '8020', '13', '562', '845', '682', '151', '3483', '12031', '238', '3578', '469', '8020', '221', '863', '678', '151', '3274', '203', '14641', '204', '146', '7629', '13', '477', '469', '8020', '11267', '173', '1839', '502', '2490', '173', '9089', '11', '1310', '13', '12703', '267', '3919', '6640', '333', '25392', '173', '641', '223', '1475', '13', '530', '203', '3669', '173', '196', '10664', '151', '3274', '5558', '174', '13617', '13', '469', '8020', '10632', '3073', '366', '240', '207', '213', '23833', '11', '1324', '173', '5842', '333', '4218', '13', '255', '573', '11', '8336', '3872', '287', '177', '347', '2224', '260', '238', '172', '8020', '221', '13596', '316', '8', '1551', '466', '12084', '175', '299', '11', '342', '351', '3367', '1451', '6951', '13', '146', '990', '1291', '1324', '11', '17871', '2821', '221', '11844', '2278', '469', '8020', '173', '1543', '17871', '2821', '213', '467', '561', '45', '276', '22645', '175', '21933', '13534', '3561', '236', '351', '2605', '11', '17871', '2821', '7427', '2449', '513', '467', '1431', '1092', '173', '1543', '469', '8020', '13', '956', '377', '7304', '11', '17871', '2821', '3920', '469', '8020', '211', '562', '494', '379', '175', '151', '348', '336', '447', '2252', '11', '530', '348', '11464', '151', '1974', '1672', '173', '469', '8020', '13', '469', '8020', '203', '642', '1742', '287', '17871', '2821', '221', '11844', '173', '146', '1091', '1217', '151', '14709', '176', '2462', '175', '333', '1304', '9089', '13', '222', '14724', '211', '9089', '336', '447', '3326', '173', '527', '204', '151', '174', '13617', '11', '342', '203', '3451', '11280', '13', '469', '8020', '1324', '1551', '146', '1463', '232', '151', '3503', '9574', '11', '146', '1220', '175', '12039', '24854', '9574', '2617', '173', '151', '1807', '287', '151', '168', '6894', '11', '173', '841', '539', '173', '11741', '19081', '221', '5869', '318', '11', '407', '351', '1658', '203', '2187', '146', '17513', '829', '4057', '1375', '18525', '17871', '2821', '221', '8196', '13', '204', '151', '3503', '9574', '6', '3274', '11', '469', '8020', '7200', '146', '5314', '293', '1310', '13', '12703', '267', '3919', '11', '407', '11526', '173', '196', '146', '7867', '3483', '1956', '13', '232', '299', '11', '222', '25679', '232', '179', '5197', '788', '1310', '13', '12703', '267', '3919', '13', '222', '642', '2881', '146', '488', '10374', '7430', '1310', '13', '12703', '267', '3919', '2413', '539', '173', '1956', '9089', '13', '146', '3389', '11437', '539', '956', '151', '488', '426', '11', '179', '1324', '10961', '539', '293', '146', '2739', '232', '11741', '19081', '13', '1324', '11', '151', '9574', '1860', '469', '8020', '3439', '377', '15723', '11', '662', '222', '8071', '232', '295', '5565', '5902', '292', '42', '438', '3353', '13', '176', '4815', '173', '295', '5565', '221', '1650', '5685', '1420', '6934', '11', '530', '336', '7120', '146', '383', '246', '45', '3665', '1953', '179', '1500', '1', '22551', '204', '151', '4809', '175', '177', '347', '2224', '260', '13', '547', '477', '351', '264', '503', '1995', '204', '151', '3857', '4252', '11', '342', '595', '223', '11', '702', '530', '10545', '467', '19519', '2353', '175', '3252', '179', '5028', '13', '351', '264', '11', '232', '13422', '709', '6471', '11', '1265', '173', '3077', '151', '8196', '175', '7861', '17871', '2821', '179', '151', '829', '1366', '16845', '13', '471', '11', '469', '8020', '179', '295', '5565', '11', '19105', '236', '9574', '11', '886', '9527', '9424', '11', '2464', '165', '7682', '175', '151', '22229', '13', '631', '281', '2408', '841', '466', '173', '151', '22229', '11', '375', '203', '151', '1974', '494', '175', '151', '348', '13', '351', '264', '17100', '176', '377', '1867', '173', '5088', '287', '11741', '19081', '11', '375', '16240', '211', '940', '175', '466', '348', '1636', '173', '1261', '151', '444', '287', '1457', '175', '395', '4359', '547', '13', '469', '8020', '7416', '333', '573', '1636', '11', '642', '13574', '2639', '11741', '19081', '179', '259', '546', '333', '994', '13', '9089', '11', '375', '203', '11741', '19081', '221', '11661', '11', '642', '11737', '223', '513', '146', '11795', '168', '15509', '5507', '1800', '11', '407', '21259', '223', '13', '2001', '333', '1608', '11', '151', '829', '1366', '4723', '173', '7199', '13', '2389', '151', '24087', '11', '469', '8020', '179', '344', '333', '2044', '238', '3578', '151', '5070', '175', '151', '12320', '3073', '366', '240', '207', '8', '264', '1265', '173', '5088', '13', '7861', '17871', '2821', '25006', '293', '467', '1075', '13772', '58', '11', '642', '7677', '236', '146', '1058', '175', '698', '17858', '287', '151', '829', '1366', '11', '407', '414', '3757', '146', '5961', '173', '1800', '13', '469', '8020', '11', '637', '177', '3705', '175', '151', '3857', '4252', '11', '4236', '52', '3409', '1194', '162', '235', '236', '333', '12976', '179', '12703', '267', '3919', '236', '333', '24781', '13', '1022', '25', '505', '36', '315', '42', '153', '382', '7509', '635', '1478', '183', '25', '12951', '141', '12951', '85', '12951', '135', '18799', '116', '12951', '141', '12951', '126', '12951', '128', '12951', '122', '12951', '79', '12951', '133', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305']]\n"
     ]
    }
   ],
   "source": [
    "bookwords = []\n",
    "#for s in df.summary:\n",
    "#    bookwords.append(list(tokenize(s, lowercase=True)))\n",
    "for s in df.mapped_inputs:\n",
    "    bookwords.append([str(i) for i in s])\n",
    "print(bookwords[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kacper/ecoNLP/lightning_class.ipynb Cell 41\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdel\u001b[39;00m df\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#Y123sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdel\u001b[39;00m train_df\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#Y123sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdel\u001b[39;00m val_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "del df\n",
    "del train_df\n",
    "del val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSimple(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.name='Word2VecSimple'\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity='none'\n",
    "        self.w2v_init=True\n",
    "        self.num_layers=0\n",
    "        self.bidirectional=False\n",
    "        self.bimult=1+self.bidirectional\n",
    "\n",
    "        w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "        self.emb = nn.Embedding(vocab_size, hid_dim, padding_idx=-1)\n",
    "\n",
    "        emb_lst = []\n",
    "        for v in range(vocab_size):\n",
    "            emb_lst.append(w2vmodel.wv[str(v)])\n",
    "        \n",
    "        emb_mat = np.array(emb_lst)\n",
    "        self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "        # load embeddings from pretrained word2vec\n",
    "        #self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1)\n",
    "        #self.out_layer=nn.Linear(hid_dim, out_dim)\n",
    "        self.out_layer=nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "                                     nn.Linear(hid_dim, hid_dim),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.Linear(hid_dim, out_dim)\n",
    "                                     )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, lengths = x\n",
    "\n",
    "        w2v_output = self.emb(inputs)\n",
    "        #avg_output = w2v_output.mean(dim=1)\n",
    "        avg_output = torch.stack([w2v_output[i, :lengths[i]].mean(dim=0) for i in range(lengths.shape[0])])\n",
    "       # print(f\"w2v_output:{w2v_output.shape}\")\n",
    "        #print(f\"avg_output:{avg_output.shape}\")\n",
    "        return self.out_layer(avg_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_simple_model = BookGenreClassifier(Word2VecSimple(256, 10, vocab_size=vocab_size), \n",
    "                             loss=nn.CrossEntropyLoss(),#weight=torch.tensor(label_weights, dtype=torch.float32)),\n",
    "                             lr_dc=0.1,\n",
    "                             lr_dc_step=4,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=False\n",
    "    )\n",
    "\n",
    "trainer=pl.Trainer(max_epochs=50,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "         #   LearningRateFinder(min_lr=1e-4, num_training_steps=1000)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkpuchalskixiv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240617_001342-kqah8woa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/kqah8woa' target=\"_blank\">toasty-flower-82</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/kqah8woa' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/kqah8woa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Word2VecSimple   | 6.6 M \n",
      "-------------------------------------------\n",
      "6.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.6 M     Total params\n",
      "26.222    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  35%|███▍      | 321/930 [00:07<00:13, 45.84it/s, v_num=8woa, train_loss=0.767, val_loss=0.738, val_acc=50.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████</td></tr><tr><td>scheduler_lr</td><td>▁▁▁</td></tr><tr><td>train_loss</td><td>▂▁▂▃▇▄▂▂▄▃▃▄▃▃▅▃▃▃▄▂▂▅▃▅▁▅▅▅▄▁▁▂▂▄▂▁█▁▃▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁█</td></tr><tr><td>val_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>scheduler_lr</td><td>0.01</td></tr><tr><td>train_loss</td><td>0.83049</td></tr><tr><td>trainer/global_step</td><td>2149</td></tr><tr><td>val_acc</td><td>50.15308</td></tr><tr><td>val_loss</td><td>0.73756</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-flower-82</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/kqah8woa' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/kqah8woa</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240617_001342-kqah8woa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(w2v_simple_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0, nonlinearity='tanh', w2v_init=False):\n",
    "        super().__init__()\n",
    "        self.name='SimpleRNN'\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity=nonlinearity\n",
    "        self.w2v_init=w2v_init\n",
    "        # last token states 'end of string' and is repeated multiple time at the end of an input\n",
    "        # therefore set its embedding to 0 with padding_idx=-1\n",
    "        self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1) \n",
    "        if w2v_init:\n",
    "            w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "            emb_lst = []\n",
    "            for v in range(vocab_size):\n",
    "                emb_lst.append(w2vmodel.wv[str(v)])\n",
    "            emb_mat = np.array(emb_lst)\n",
    "            self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "            \n",
    "        self.model=nn.RNN(input_size=hid_dim, \n",
    "                          hidden_size=hid_dim, \n",
    "                          batch_first=True, \n",
    "                          dropout=dropout_p,\n",
    "                         # bidirectional=True,\n",
    "                          nonlinearity=nonlinearity)\n",
    "        self.out_layer= nn.Linear(hid_dim, out_dim)\n",
    "        #nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "         #                            nn.Linear(hid_dim, hid_dim),\n",
    "          #                           nn.LeakyReLU(),\n",
    "           #                          nn.Dropout(p=0.2),\n",
    "            #                         nn.Linear(hid_dim, out_dim)\n",
    "             #                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, att_mask=x\n",
    "        rnn_input=self.emb(inputs)\n",
    "        h0 = torch.randn(1, rnn_input.shape[0], rnn_input.shape[-1], device=rnn_input.device)\n",
    "        hstates, hn = self.model(rnn_input, h0)\n",
    "       # print(h_states.shape)\n",
    "        hn[0]=torch.stack([hstates[e, int(i)-1] for e,i in enumerate(att_mask.sum(axis=1))])\n",
    "        hn=hn.view(inputs.shape[0], -1)\n",
    "        #use last hidden state as sequence representation\n",
    "     #   hn=hn.squeeze()\n",
    "        return self.out_layer(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "rnn_model=BookGenreClassifier(SimpleRNN(32, 10, vocab_size=vocab_size,\n",
    "                                        nonlinearity='tanh',\n",
    "                                        dropout_p=0.5,\n",
    "                                        w2v_init=True,\n",
    "                                        ), \n",
    "                             loss=nn.CrossEntropyLoss(),#weight=torch.tensor(label_weights, dtype=torch.float32),\n",
    "                             lr=1e-3,\n",
    "                             lr_dc=0.5,\n",
    "                             lr_dc_step=4,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=20,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "         #   LearningRateFinder(min_lr=1e-4, num_training_steps=1000)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240616_192615-ggibuo4k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k' target=\"_blank\">different-frog-47</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | SimpleRNN        | 1.2 M \n",
      "-------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.660     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 130/130 [00:05<00:00, 22.49it/s, v_num=uo4k, train_loss=0.897, val_loss=2.350, val_acc=25.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 130/130 [00:05<00:00, 22.48it/s, v_num=uo4k, train_loss=0.897, val_loss=2.350, val_acc=25.30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>scheduler_lr</td><td>████████████▃▃▃▃▃▃▁▁</td></tr><tr><td>train_loss</td><td>█▇▇█▇█▇▇▇▇▆▇▆▆▆▇▆▆▅▅▃▅▄▄▄▃▃▃▃▄▄▂▃▂▁▁▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▃▄▁▃▄▅▅▆▆▄▆▅▆█▇▆▄▆▄</td></tr><tr><td>val_loss</td><td>▂▁▁▁▁▁▁▁▁▂▃▃▄▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>scheduler_lr</td><td>0.00025</td></tr><tr><td>train_loss</td><td>0.89707</td></tr><tr><td>trainer/global_step</td><td>2599</td></tr><tr><td>val_acc</td><td>25.32189</td></tr><tr><td>val_loss</td><td>2.35457</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-frog-47</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240616_192615-ggibuo4k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(rnn_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▆▆▆▁▁▂▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇███</td></tr><tr><td>scheduler_lr</td><td>███████▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▇███▇▇▆▆▆▆▆▅▅▄▅█▇███▇▆▅▇▄▄▃▄▃▂▂▂▂▂▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▆▆▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>val_acc</td><td>▂▄▄▂▃▃▁▂▃▄▃▄▃▂▆▄▄▆▇▆█▅▆▆▄▅▅▄▄▃▃</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▂▂▂▃▃▃▄▁▁▁▁▁▁▁▂▃▄▄▅▆▆▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>scheduler_lr</td><td>0.00025</td></tr><tr><td>train_loss</td><td>0.14059</td></tr><tr><td>trainer/global_step</td><td>2339</td></tr><tr><td>val_acc</td><td>22.103</td></tr><tr><td>val_loss</td><td>4.35297</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-frost-41</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/1tthcp5y' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/1tthcp5y</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240616_190118-1tthcp5y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0, nonlinearity='none', w2v_init=False, num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.name='SimpleLSTM'\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity=nonlinearity\n",
    "        self.w2v_init=w2v_init\n",
    "        self.num_layers=num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.bimult=1+bidirectional\n",
    "        # last token states 'end of string' and is repeated multiple time at the end of an input\n",
    "        # therefore set its embedding to 0 with padding_idx=-1\n",
    "        self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1) \n",
    "        if w2v_init:\n",
    "            w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "            emb_lst = []\n",
    "            for v in range(vocab_size):\n",
    "                emb_lst.append(w2vmodel.wv[str(v)])\n",
    "            emb_mat = np.array(emb_lst)\n",
    "            self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "\n",
    "        self.model=nn.LSTM(input_size=hid_dim, \n",
    "                           hidden_size=hid_dim, \n",
    "                           batch_first=True, \n",
    "                           dropout=dropout_p, \n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional)\n",
    "      #  self.out_layer=nn.Linear(self.bimult*num_layers*hid_dim, out_dim)\n",
    "        self.out_layer=nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "                                     nn.Linear(self.bimult*num_layers*hid_dim+hid_dim, hid_dim),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.Linear(hid_dim, out_dim)\n",
    "                                     )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, lengths=x\n",
    "        rnn_input=self.emb(inputs)\n",
    "\n",
    "     #   avg_emb = torch.stack([rnn_input[i, :lengths[i]].mean(dim=0) for i in range(lengths.shape[0])])\n",
    "\n",
    "        h0 = torch.randn(self.bimult*self.num_layers, rnn_input.shape[0], rnn_input.shape[-1], device=rnn_input.device)\n",
    "        c0 = torch.randn(self.bimult*self.num_layers, rnn_input.shape[0], rnn_input.shape[-1], device=rnn_input.device)\n",
    "\n",
    "        rnn_input=pack_padded_sequence(rnn_input,lengths.to('cpu').to(int), batch_first=True, enforce_sorted=False)\n",
    "        hstates, (hn, _) = self.model(rnn_input, (h0, c0))\n",
    "        padded_hstates, lengths=[x.to(inputs.device) for x in pad_packed_sequence(hstates, batch_first=True)]\n",
    "        hstates_avg=padded_hstates.sum(dim=1).div(lengths.float().unsqueeze(dim=1))\n",
    "        #return hstates\n",
    "       # print(h_states.shape)\n",
    "    #    hn[0]=torch.stack([hstates[e, int(i)-1] for e,i in enumerate(lengths)])\n",
    "        hn=hn.view(inputs.shape[0], -1)\n",
    "       # hn=hn.squeeze()\n",
    "       #return self.out_layer(hn)\n",
    "        return self.out_layer(torch.concat([hstates_avg, hn], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "lstm_model=BookGenreClassifier(SimpleLSTM(256, 10, vocab_size=vocab_size,\n",
    "                                       # nonlinearity='tanh',\n",
    "                                        dropout_p=0.5,\n",
    "                                        w2v_init=True,\n",
    "                                        num_layers=1,\n",
    "                                        ), \n",
    "                             loss=nn.CrossEntropyLoss(),#weight=torch.tensor(label_weights, dtype=torch.float32)),\n",
    "                             lr=1e-4,\n",
    "                             lr_dc=0.5,\n",
    "                             lr_dc_step=4,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=50,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "          #  LearningRateFinder(num_training_steps=1000)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240617_010734-2pgkq94r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/2pgkq94r' target=\"_blank\">ethereal-oath-86</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/2pgkq94r' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/2pgkq94r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | SimpleLSTM       | 7.4 M \n",
      "-------------------------------------------\n",
      "7.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.4 M     Total params\n",
      "29.578    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 130/130 [00:18<00:00,  7.01it/s, v_num=q94r, train_loss=1.290, val_loss=1.470, val_acc=43.80]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>scheduler_lr</td><td>██████▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▅▇█▄▅▇▃▃▅▃▄▅▄▆▂▂▄▅▃▆▃▄▁▃▃▁▄▂▄▄▃▁▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▃▄▄▅▅▆▆▆▇▇▆▆▇██▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▇▅▅▄▃▃▃▂▂▂▁▂▂▂▁▁▂▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>scheduler_lr</td><td>1e-05</td></tr><tr><td>train_loss</td><td>1.28331</td></tr><tr><td>trainer/global_step</td><td>3379</td></tr><tr><td>val_acc</td><td>43.77682</td></tr><tr><td>val_loss</td><td>1.47321</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-oath-86</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/2pgkq94r' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/2pgkq94r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240617_010734-2pgkq94r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lstm_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-jazz-60</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/8jy266yv' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/8jy266yv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240616_212924-8jy266yv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0, nonlinearity='none', w2v_init=False, num_layers=1, bidirectional=False,\n",
    "                 nheads=8,):\n",
    "        super().__init__()\n",
    "        self.name='SimpleTransformer'\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity=nonlinearity\n",
    "        self.w2v_init=w2v_init\n",
    "        self.num_layers=num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.nheads=nheads\n",
    "        # last token states 'end of string' and is repeated multiple time at the end of an input\n",
    "        # therefore set its embedding to 0 with padding_idx=-1\n",
    "        self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1) \n",
    "        if w2v_init:\n",
    "            w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "            emb_lst = []\n",
    "            for v in range(vocab_size):\n",
    "                emb_lst.append(w2vmodel.wv[str(v)])\n",
    "            emb_mat = np.array(emb_lst)\n",
    "            self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "\n",
    "        t_layer= nn.TransformerEncoderLayer(d_model=hid_dim, nhead=nheads, batch_first=True, dropout=dropout_p)\n",
    "        self.model= nn.TransformerEncoder(t_layer, num_layers=num_layers)\n",
    "      #  self.out_layer=nn.Linear(self.bimult*num_layers*hid_dim, out_dim)\n",
    "        self.out_layer=nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "                                     nn.Linear(hid_dim+hid_dim, hid_dim),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.Linear(hid_dim, out_dim)\n",
    "                                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inputs, lengths=x\n",
    "        max_len=min(512, torch.max(lengths))\n",
    "        t_input=self.emb(inputs[:, :max_len])\n",
    "\n",
    "        hstates=self.model(t_input)\n",
    "        hstates_avg=hstates.sum(dim=1).div(lengths.float().unsqueeze(dim=1))\n",
    "        hn=hstates[:,-1]\n",
    "\n",
    "        return self.out_layer(torch.concat([hstates_avg, hn], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model=BookGenreClassifier(TransformerModel(256, 10, vocab_size=vocab_size,\n",
    "                                       # nonlinearity='tanh',\n",
    "                                        dropout_p=0.5,\n",
    "                                        w2v_init=True,\n",
    "                                        num_layers=2,\n",
    "                                        ), \n",
    "                             loss=nn.CrossEntropyLoss(),#weight=torch.tensor(label_weights, dtype=torch.float32)),\n",
    "                             lr=1e-4,\n",
    "                             lr_dc=0.5,\n",
    "                             lr_dc_step=4,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=50,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "          #  LearningRateFinder(num_training_steps=1000)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkpuchalskixiv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240617_013412-20vphev7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/20vphev7' target=\"_blank\">chocolate-paper-88</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/20vphev7' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/20vphev7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | TransformerModel | 9.5 M \n",
      "-------------------------------------------\n",
      "9.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.5 M     Total params\n",
      "37.994    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  30%|███       | 39/130 [00:04<00:10,  8.72it/s, v_num=hev7, train_loss=1.510, val_loss=1.760, val_acc=31.50] "
     ]
    }
   ],
   "source": [
    "trainer.fit(lstm_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_stuff",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
