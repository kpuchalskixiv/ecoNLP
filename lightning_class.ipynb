{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "from os import cpu_count\n",
    "from types import NoneType\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint, LearningRateFinder\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'genre', 'summary', 'input_ids', 'att_mask', 'label',\n",
       "       'mapped_inputs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_parquet('./data/books.par')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(data_utils.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.input_ids=torch.tensor(df.mapped_inputs, dtype=torch.int32)\n",
    "        self.lengths=torch.tensor(df.att_mask.map(sum), dtype=torch.int32)\n",
    "        self.y=torch.tensor(df.label.values, dtype=torch.uint8)\n",
    "        self.no_classes=df.label.nunique()\n",
    "        self.max_len=self.input_ids.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indexes):\n",
    "        return self.input_ids[indexes], self.lengths[indexes], self.y[indexes]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedDataset(data_utils.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.input_ids=torch.tensor(df.mapped_inputs, dtype=torch.int32)\n",
    "        self.lengths=torch.tensor(df.att_mask.map(sum), dtype=torch.int32)\n",
    "        self.y=torch.tensor(df.label.values, dtype=torch.uint8)\n",
    "        self.no_classes=df.label.nunique()\n",
    "        self.max_len=self.input_ids.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indexes):\n",
    "       \n",
    "        return pack_padded_sequence(self.input_ids[indexes].view(-1, self.max_len), \n",
    "                                    self.lengths[indexes].view(-1), \n",
    "                                    batch_first=True, enforce_sorted=False), self.y[indexes]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_elementwise_apply(fn, packed_sequence):\n",
    "    \"\"\"applies a pointwise function fn to each element in packed_sequence\"\"\"\n",
    "    return torch.nn.utils.rnn.PackedSequence(fn(packed_sequence.data), packed_sequence.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pack_padded_sequence(emb(b[0]), b[1], batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_elementwise_apply(emb, val_data[:32][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[np.random.permutation(df.shape[0])].reset_index(drop=True)\n",
    "split=int(df.shape[0]*0.9)\n",
    "\n",
    "train_df=df.iloc[:split]\n",
    "val_df=df.iloc[split:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02317972, 0.03169227, 0.04112196, 0.03423001, 0.03329985,\n",
       "       0.02000165, 0.1997991 , 0.17846133, 0.2112818 , 0.22693231])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,label_weights=np.unique(train_df.label, return_counts=True)\n",
    "label_weights=1/label_weights\n",
    "label_weights=label_weights/np.sum(label_weights)\n",
    "label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102037/2633636942.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  self.input_ids=torch.tensor(df.mapped_inputs, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "train_data=BookDataset(train_df)\n",
    "val_data=BookDataset(val_df)\n",
    "\n",
    "train_dataloader=data_utils.DataLoader(train_data, batch_size=32, num_workers=cpu_count(),\n",
    "                                       shuffle=True, drop_last=True)\n",
    "val_dataloader=data_utils.DataLoader(val_data, batch_size=32, num_workers=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pl module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookGenreClassifier(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-2, loss=nn.CrossEntropyLoss(), l2=1e-5, lr_dc_step=3, lr_dc=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model','loss'])\n",
    "        if isinstance(loss.weight, NoneType):\n",
    "            weighted_loss=False\n",
    "        else: weighted_loss=True\n",
    "        self.save_hyperparameters({'name':model.name, \n",
    "                                   'dropout_p':model.dropout_p,\n",
    "                                   'w2v_init':model.w2v_init,\n",
    "                                   'num_layers':model.num_layers,\n",
    "                                   'nonlinearity':model.nonlinearity,\n",
    "                                   'bidirectional':model.bidirectional,\n",
    "                                   'weighted_loss':weighted_loss})\n",
    "        self.lr=lr\n",
    "        self.loss=loss\n",
    "        self.model=model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[:-1]\n",
    "        y = batch[-1]\n",
    "        logits=self(x)\n",
    "        loss=self.loss(logits, y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, batch, mode=None):\n",
    "        x = batch[:-1]\n",
    "        y = batch[-1]\n",
    "        logits=self(x)\n",
    "\n",
    "        loss=self.loss(logits, y)\n",
    "\n",
    "        preds=torch.argmax(logits, axis=1)\n",
    "        acc=torch.sum(preds==y)/y.shape[0]\n",
    "        # TODO add more metrics\n",
    "\n",
    "        if mode:\n",
    "            self.log(mode+'_loss', loss,  prog_bar=True)\n",
    "            self.log(mode+'_acc', 100*acc,  prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        return self.evaluate(batch, \"val\")\n",
    "    def test_step(self, batch, *args, **kwargs):\n",
    "        return self.evaluate(batch, \"test\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.hparams.l2\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            patience=self.hparams.lr_dc_step,\n",
    "            factor=self.hparams.lr_dc,\n",
    "            cooldown=1,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_acc\",\n",
    "                \"strict\": False,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"name\": \"scheduler_lr\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module): \n",
    "    def __init__(self, in_dim=7031, hid_dim=128, out_dim=10):\n",
    "        # dummy model as an example, just one hidden layer straight from list of tokens\n",
    "        super().__init__()\n",
    "        self.name='DummyModel'\n",
    "        self.dropout_p=0\n",
    "        self.l1=nn.Linear(in_dim, hid_dim)\n",
    "        self.nonlinear=nn.Tanh()\n",
    "        self.l2=nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_ids, att_mask = x\n",
    "        x=in_ids*att_mask\n",
    "        x=self.l1(x)\n",
    "        x=self.nonlinear(x)\n",
    "        return self.l2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_model=BookGenreClassifier(DummyModel(), loss=nn.CrossEntropyLoss(weight=torch.tensor(label_weights, dtype=torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/szymon/pythonvenvs/rocmwork/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0      | train\n",
      "1 | model | DummyModel       | 901 K  | train\n",
      "---------------------------------------------------\n",
      "901 K     Trainable params\n",
      "0         Non-trainable params\n",
      "901 K     Total params\n",
      "3.606     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a7ad7fdae44ddbaf25d5bd4179b128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                   | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(dm_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  981,  1259, 17566,  ..., 26305, 26305, 26305], dtype=torch.int32),\n",
       " tensor(1910, dtype=torch.int32),\n",
       " tensor(4, dtype=torch.uint8))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26306"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_set=set()\n",
    "for tokens in df.mapped_inputs.values:\n",
    "    tokens_set=tokens_set.union(set(tokens))\n",
    "vocab_size=len(tokens_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['981', '1259', '17566', '934', '3160', '11', '940', '395', '7182', '3811', '255', '151', '573', '10990', '11', '342', '351', '264', '176', '929', '4553', '13', '146', '12320', '17781', '2400', '9012', '3153', '173', '1854', '527', '11', '170', '939', '18286', '252', '865', '2139', '6216', '20474', '287', '5812', '2403', '179', '4990', '167', '466', '13', '236', '222', '4418', '146', '1568', '2422', '238', '1295', '162', '678', '375', '336', '447', '1217', '287', '590', '20474', '682', '151', '15521', '11', '222', '13040', '146', '1568', '2011', '11', '153', '11409', '45', '1898', '11', '375', '336', '146', '2659', '23466', '175', '8745', '224', '1702', '467', '13', '170', '939', '5478', '151', '20474', '375', '294', '447', '3654', '193', '5362', '232', '377', '988', '179', '11', '236', '351', '684', '338', '294', '146', '1711', '4607', '11', '9096', '395', '175', '466', '11', '817', '151', '444', '238', '34', '3254', '8', '2480', '1264', '13', '222', '4538', '173', '151', '1486', '1956', '7351', '11', '662', '333', '4957', '11', '164', '21286', '2941', '154', '8085', '355', '11', '18300', '539', '211', '222', '570', '196', '176', '2860', '236', '1659', '11331', '150', '238', '1889', '175', '151', '930', '1956', '8', '1050', '621', '10853', '213', '333', '3237', '176', '4080', '2649', '1563', '179', '2925', '539', '146', '14367', '22295', '1217', '16126', '3889', '238', '6476', '1324', '18776', '513', '146', '2011', '678', '213', '333', '3814', '13', '170', '939', '5535', '17354', '151', '2395', '11', '642', '5478', '16126', '3889', '176', '333', '5970', '179', '20539', '10037', '13', '151', '1100', '930', '222', '10705', '2881', '333', '4389', '213', '764', '238', '1289', '4374', '146', '873', '221', '12099', '11', '146', '3827', '1217', '246', '2304', '5236', '1381', '3914', '179', '13721', '232', '146', '2649', '444', '293', '151', '930', '1956', '11', '306', '6761', '684', '11629', '7370', '13', '351', '3396', '211', '306', '6761', '329', '633', '467', '988', '173', '334', '146', '3343', '4753', '575', '236', '170', '4075', '213', '467', '7031', '410', '299', '1922', '13', '222', '642', '16953', '211', '16126', '3889', '329', '2269', '179', '203', '146', '19428', '461', '9586', '176', '170', '22295', '221', '1494', '236', '146', '7224', '13', '170', '939', '2416', '173', '193', '5362', '221', '5970', '173', '1525', '539', '293', '151', '15230', '11', '642', '2484', '10248', '539', '236', '193', '5362', '17782', '19828', '176', '151', '24499', '213', '151', '573', '498', '13', '8796', '11', '222', '3070', '15903', '18893', '931', '173', '1401', '268', '173', '2649', '13', '10417', '11', '164', '21286', '2941', '154', '8085', '355', '9071', '170', '16013', '260', '11', '2941', '154', '11', '173', '151', '15836', '2011', '11', '153', '11409', '45', '1898', '188', '17854', '7370', '11', '173', '863', '467', '6623', '179', '754', '375', '570', '294', '15836', '467', '11', '342', '173', '503', '17713', '13', '2807', '11', '170', '939', '203', '9012', '3153', '173', '2621', '153', '11409', '45', '1898', '11', '179', '176', '151', '9999', '11', '151', '282', '552', '42', '461', '3108', '173', '193', '5362', '179', '222', '10632', '467', '204', '151', '7168', '13', '170', '939', '10554', '173', '886', '370', '211', '153', '11409', '45', '1898', '203', '146', '2972', '444', '375', '17782', '15836', '4114', '232', '467', '10350', '238', '1386', '502', '530', '606', '173', '467', '2318', '593', '170', '939', '642', '2480', '592', '173', '193', '5362', '221', '7168', '173', '886', '151', '282', '552', '42', '461', '179', '193', '5362', '11', '236', '725', '236', '712', '444', '1401', '179', '2649', '16851', '11', '1172', '170', '939', '221', '8736', '11', '346', '324', '3194', '340', '1293', '4042', '11', '375', '203', '146', '15230', '13', '1659', '11331', '150', '10632', '466', '179', '2799', '170', '939', '13', '342', '477', '222', '702', '11', '395', '175', '151', '1401', '24511', '11', '115', '665', '34', '11', '7368', '1877', '173', '196', '164', '21286', '2941', '154', '8085', '355', '11', '375', '1307', '21268', '377', '4579', '13', '817', '1659', '11331', '150', '221', '2743', '1080', '12674', '11', '179', '193', '5362', '4902', '333', '2900', '213', '1401', '11', '176', '146', '780', '2335', '11', '306', '6761', '2881', '467', '3506', '232', '170', '939', '173', '641', '539', '1317', '193', '5362', '1905', '13', '170', '939', '7368', '173', '193', '5362', '211', '222', '250', '146', '17961', '812', '287', '164', '21286', '2941', '154', '8085', '355', '179', '1659', '11331', '150', '176', '377', '9345', '788', '13', '19020', '11', '170', '939', '4409', '151', '7168', '11', '3376', '2458', '1251', '13', '146', '2649', '444', '11', '6948', '1211', '542', '7042', '34', '11', '203', '2415', '287', '9187', '11', '146', '8219', '17788', '1063', '170', '23314', '9509', '18741', '11', '7171', '173', '1877', '236', '246', '981', '3997', '1602', '151', '1100', '930', '11', '164', '21286', '2941', '154', '8085', '355', '16458', '211', '151', '930', '1956', '8474', '395', '175', '151', '1486', '1956', '16851', '13', '2077', '1948', '173', '294', '170', '306', '17001', '11', '2355', '170', '939', '13', '164', '21286', '2941', '154', '8085', '355', '5217', '211', '170', '939', '336', '447', '744', '376', '287', '151', '930', '1956', '11', '179', '2881', '333', '4389', '173', '10982', '170', '939', '179', '16126', '3889', '221', '4579', '238', '231', '3889', '698', '637', '9442', '11880', '593', '176', '16126', '3889', '221', '1494', '11', '170', '939', '642', '2416', '232', '153', '11409', '45', '1898', '173', '467', '5970', '173', '5869', '11', '179', '11', '562', '222', '2480', '471', '11', '7368', '211', '222', '203', '176', '929', '170', '939', '11', '1388', '153', '11409', '45', '1898', '14135', '236', '530', '512', '1093', '246', '231', '3889', '1', '530', '4733', '170', '939', '13', '351', '335', '173', '146', '5376', '1324', '176', '151', '4752', '662', '351', '3325', '170', '176', '24331', '2649', '444', '232', '333', '1387', '13', '151', '2649', '444', '2416', '173', '151', '11026', '11', '662', '9187', '250', '3921', '179', '9096', '539', '13', '170', '939', '11', '7982', '1285', '151', '2649', '444', '414', '279', '3595', '293', '151', '11026', '11', '8322', '4409', '153', '11409', '45', '1898', '179', '16953', '151', '1494', '13', '164', '21286', '2941', '154', '8085', '355', '642', '7368', '539', '5357', '179', '5924', '153', '11409', '45', '1898', '173', '1333', '539', '1675', '211', '570', '863', '173', '5698', '151', '8501', '11', '179', '153', '11409', '45', '1898', '4463', '5313', '961', '9071', '539', '151', '2426', '175', '9187', '221', '2964', '221', '14480', '13', '1659', '11331', '150', '642', '1727', '484', '179', '2486', '2233', '170', '939', '176', '16126', '3889', '221', '1494', '11', '8356', '539', '232', '151', '4047', '13', '170', '939', '4032', '163', '817', '1659', '11331', '150', '6262', '173', '1903', '539', '232', '170', '3256', '11888', '988', '11', '179', '10554', '173', '20763', '146', '917', '13', '502', '222', '1348', '338', '616', '203', '211', '151', '2600', '176', '151', '917', '264', '9187', '179', '333', '2964', '13', '4737', '1149', '11', '170', '939', '3108', '16126', '3889', '179', '5924', '173', '4012', '4579', '592', '13', '3921', '176', '151', '12975', '213', '16126', '3889', '11', '170', '939', '220', '15912', '2001', '193', '5362', '179', '336', '146', '3596', '4146', '232', '539', '13', '16126', '3889', '10068', '179', '170', '939', '179', '16126', '3889', '4012', '377', '4579', '592', '1063', '170', '609', '286', '224', '211', '7368', '164', '21286', '2941', '154', '8085', '355', '221', '923', '1212', '236', '197', '12623', '13', '817', '2761', '173', '16126', '3889', '11', '170', '939', '923', '2233', '211', '151', '930', '1956', '203', '547', '13196', '539', '176', '1269', '173', '641', '153', '11409', '45', '1898', '6520', '179', '633', '467', '4389', '10705', '11', '407', '424', '1054', '466', '173', '20039', '175', '467', '11', '268', '255', '1308', '21838', '8599', '467', '13', '170', '939', '642', '2278', '146', '5041', '176', '151', '15521', '179', '477', '222', '2416', '434', '255', '146', '3540', '11', '16313', '287', '146', '2649', '444', '11', '146', '24499', '3064', '11', '146', '15288', '17781', '11', '6805', '211', '170', '939', '667', '335', '173', '151', '15070', '802', '2384', '7690', '13', '222', '9096', '151', '2649', '17781', '13196', '539', '179', '2881', '333', '4357', '173', '6887', '1877', '13', '222', '10068', '255', '151', '7690', '173', '886', '211', '151', '930', '1956', '336', '3898', '377', '6373', '7351', '176', '151', '7690', '13', '11730', '22986', '2187', '11', '222', '5475', '16351', '9569', '236', '170', '15320', '293', '151', '1401', '179', '146', '5782', '175', '19827', '2649', '24511', '13722', '333', '2462', '13', '3450', '151', '7690', '11', '170', '939', '2416', '287', '193', '5362', '221', '1807', '179', '5217', '1659', '11331', '150', '203', '1893', '10853', '204', '539', '287', '3692', '539', '376', '13', '10417', '11', '9187', '4168', '151', '3906', '175', '146', '2649', '698', '179', '2416', '204', '146', '8942', '13', '222', '2807', '5478', '151', '2649', '698', '179', '203', '22803', '477', '222', '16953', '223', '203', '1568', '193', '5362', '13', '170', '939', '7470', '466', '179', '4716', '232', '9187', '11', '8063', '151', '10823', '850', '1401', '179', '2649', '11', '342', '9187', '1348', '338', '8578', '11', '7708', '370', '211', '193', '5362', '348', '1407', '376', '173', '196', '146', '2649', '17781', '11', '179', '223', '221', '1151', '173', '1261', '539', '637', '13', '170', '939', '9871', '163', '477', '9187', '6262', '173', '1261', '193', '5362', '11', '179', '351', '1598', '176', '151', '24499', '13', '170', '939', '15091', '211', '4080', '9187', '424', '1362', '211', '344', '8221', '3577', '333', '15369', '424', '196', '2183', '11', '179', '203', '13990', '287', '9187', '13', '197', '12623', '642', '1727', '484', '11', '4395', '211', '9187', '667', '1451', '146', '2390', '175', '151', '5959', '7338', '11', '179', '477', '170', '939', '710', '3058', '410', '559', '222', '370', '56', '1823', '1659', '11331', '150', '613', '11', '197', '12623', '7368', '211', '1659', '11331', '150', '336', '1800', '173', '334', '232', '223', '179', '211', '344', '211', '250', '4648', '287', '151', '1486', '1956', '173', '4166', '153', '11409', '45', '1898', '221', '7885', '1047', '13', '170', '1239', '442', '10068', '293', '218', '22592', '3297', '179', '203', '18055', '287', '146', '896', '175', '2649', '1563', '2435', '287', '306', '6761', '11', '375', '2799', '539', '2929', '222', '14340', '146', '20216', '15326', '13', '236', '351', '1598', '11', '333', '2756', '19828', '1264', '19794', '232', '151', '15326', '13', '344', '175', '151', '1486', '1956', '16851', '335', '173', '16351', '9569', '221', '1807', '173', '6623', '11', '342', '170', '939', '1348', '338', '5108', '173', '294', '1060', '11', '236', '222', '203', '4109', '232', '153', '11409', '45', '1898', '221', '3183', '4389', '19291', '377', '2295', '179', '151', '1469', '197', '12623', '1599', '466', '434', '13', '222', '2807', '4409', '179', '477', '222', '1727', '592', '173', '333', '5970', '11', '222', '16953', '1659', '11331', '150', '18218', '2890', '146', '5680', '179', '3921', '213', '539', '13', '1659', '11331', '150', '7368', '211', '306', '3831', '11', '151', '1568', '442', '293', '218', '22592', '3297', '11', '2617', '232', '539', '170', '15326', '11', '146', '2999', '175', '18526', '13', '170', '939', '221', '1896', '4395', '211', '151', '18526', '175', '7339', '179', '211', '223', '570', '196', '812', '287', '151', '1401', '173', '17290', '14594', '11', '4019', '1785', '173', '1254', '151', '829', '173', '3574', '146', '507', '829', '1269', '13', '7315', '232', '16126', '3889', '179', '1324', '197', '12623', '5842', '211', '153', '11409', '45', '1898', '176', '929', '203', '173', '633', '151', '18526', '173', '17290', '146', '14594', '13', '4737', '1964', '11', '170', '939', '24073', '151', '1401', '988', '293', '344', '175', '151', '24463', '12', '1289', '222', '5475', '11', '1893', '377', '6466', '1264', '13', '170', '939', '10632', '197', '12623', '11', '153', '11409', '45', '1898', '11', '1659', '11331', '150', '11', '193', '5362', '11', '179', '9187', '204', '146', '19845', '662', '153', '11409', '45', '1898', '16221', '173', '17290', '146', '14594', '11', '817', '146', '4896', '203', '8101', '908', '466', '13', '153', '11409', '45', '1898', '642', '7200', '151', '1259', '175', '14594', '13', '197', '12623', '654', '3688', '170', '939', '570', '633', '344', '151', '2127', '222', '336', '18005', '173', '1881', '151', '4896', '11', '342', '170', '939', '2881', '223', '2022', '204', '1877', '2389', '146', '2340', '668', '5236', '1381', '3827', '13', '22803', '11', '153', '11409', '45', '1898', '7270', '26062', '193', '5362', '221', '14594', '179', '5924', '170', '939', '213', '4373', '11', '342', '170', '939', '954', '211', '530', '1073', '4247', '502', '173', '2886', '4114', '13', '151', '1259', '17176', '11', '179', '197', '12623', '13040', '530', '1199', '338', '2886', '1675', '11', '530', '547', '19723', '1049', '13', '193', '5362', '191', '15276', '293', '146', '2302', '2649', '444', '592', '173', '170', '15903', '18893', '993', '13', '1659', '11331', '150', '13040', '211', '377', '4648', '3887', '3259', '634', '175', '153', '11409', '45', '1898', '221', '17692', '160', '4989', '179', '11', '23943', '11', '4409', '13', '170', '939', '642', '13040', '151', '18526', '153', '11409', '45', '1898', '812', '203', '279', '1834', '13', '197', '12623', '7368', '211', '153', '11409', '45', '1898', '26062', '193', '5362', '221', '14594', '250', '512', '146', '15293', '11', '179', '176', '151', '1362', '498', '11', '16126', '3889', '191', '23535', '151', '14594', '175', '1785', '1324', '3669', '13', '197', '12623', '7368', '151', '1745', '2813', '175', '299', '1188', '250', '173', '2930', '333', '1554', '175', '16126', '3889', '13', '1037', '467', '1127', '4389', '11', '377', '1554', '250', '15420', '13', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305', '26305']]\n"
     ]
    }
   ],
   "source": [
    "bookwords = []\n",
    "#for s in df.summary:\n",
    "#    bookwords.append(list(tokenize(s, lowercase=True)))\n",
    "for s in df.mapped_inputs:\n",
    "    bookwords.append([str(i) for i in s])\n",
    "print(bookwords[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSimple(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.name='Word2VecSimple'\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity='none'\n",
    "        self.w2v_init=True\n",
    "        self.num_layers=0\n",
    "        self.bidirectional=False\n",
    "        self.bimult=1+self.bidirectional\n",
    "\n",
    "        w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "        self.emb = nn.Embedding(vocab_size, hid_dim, padding_idx=-1)\n",
    "\n",
    "        emb_lst = []\n",
    "        for v in range(vocab_size):\n",
    "            emb_lst.append(w2vmodel.wv[str(v)])\n",
    "        \n",
    "        emb_mat = np.array(emb_lst)\n",
    "        self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "        # load embeddings from pretrained word2vec\n",
    "        #self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1)\n",
    "        #self.out_layer=nn.Linear(hid_dim, out_dim)\n",
    "        self.out_layer=nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "                                     nn.Linear(hid_dim, hid_dim),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.Linear(hid_dim, out_dim)\n",
    "                                     )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, lengths = x\n",
    "\n",
    "        w2v_output = self.emb(inputs)\n",
    "        #avg_output = w2v_output.mean(dim=1)\n",
    "        avg_output = torch.stack([w2v_output[i, :lengths[i]].mean(dim=0) for i in range(lengths.shape[0])])\n",
    "       # print(f\"w2v_output:{w2v_output.shape}\")\n",
    "        #print(f\"avg_output:{avg_output.shape}\")\n",
    "        return self.out_layer(avg_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_simple_model = BookGenreClassifier(Word2VecSimple(256, 10, vocab_size=vocab_size), \n",
    "                             loss=nn.CrossEntropyLoss(weight=torch.tensor(label_weights, dtype=torch.float32)),\n",
    "                             lr_dc=0.1,\n",
    "                             lr_dc_step=4,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=False\n",
    "    )\n",
    "\n",
    "trainer=pl.Trainer(max_epochs=50,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "         #   LearningRateFinder(min_lr=1e-4, num_training_steps=1000)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkpuchalskixiv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240616_231934-9wgq9c8v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/9wgq9c8v' target=\"_blank\">driven-lion-79</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/9wgq9c8v' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/9wgq9c8v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Word2VecSimple   | 6.8 M \n",
      "-------------------------------------------\n",
      "6.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.211    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 130/130 [00:15<00:00,  8.15it/s, v_num=9c8v, train_loss=1.050, val_loss=1.370, val_acc=48.30]"
     ]
    }
   ],
   "source": [
    "trainer.fit(w2v_simple_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>scheduler_lr</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>36</td></tr><tr><td>scheduler_lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>1.89034</td></tr><tr><td>trainer/global_step</td><td>4809</td></tr><tr><td>val_acc</td><td>21.45923</td></tr><tr><td>val_loss</td><td>1.80611</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">iconic-dust-64</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/qfzvghrr' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/qfzvghrr</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240616_222530-qfzvghrr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0, nonlinearity='tanh', w2v_init=False):\n",
    "        super().__init__()\n",
    "        self.name='SimpleRNN'\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity=nonlinearity\n",
    "        self.w2v_init=w2v_init\n",
    "        # last token states 'end of string' and is repeated multiple time at the end of an input\n",
    "        # therefore set its embedding to 0 with padding_idx=-1\n",
    "        self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1) \n",
    "        if w2v_init:\n",
    "            w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "            emb_lst = []\n",
    "            for v in range(vocab_size):\n",
    "                emb_lst.append(w2vmodel.wv[str(v)])\n",
    "            emb_mat = np.array(emb_lst)\n",
    "            self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "            \n",
    "        self.model=nn.RNN(input_size=hid_dim, \n",
    "                          hidden_size=hid_dim, \n",
    "                          batch_first=True, \n",
    "                          dropout=dropout_p,\n",
    "                         # bidirectional=True,\n",
    "                          nonlinearity=nonlinearity)\n",
    "        self.out_layer= nn.Linear(hid_dim, out_dim)\n",
    "        #nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "         #                            nn.Linear(hid_dim, hid_dim),\n",
    "          #                           nn.LeakyReLU(),\n",
    "           #                          nn.Dropout(p=0.2),\n",
    "            #                         nn.Linear(hid_dim, out_dim)\n",
    "             #                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, att_mask=x\n",
    "        rnn_input=self.emb(inputs)\n",
    "        h0 = torch.randn(1, rnn_input.shape[0], rnn_input.shape[-1], device=rnn_input.device)\n",
    "        hstates, hn = self.model(rnn_input, h0)\n",
    "       # print(h_states.shape)\n",
    "        hn[0]=torch.stack([hstates[e, int(i)-1] for e,i in enumerate(att_mask.sum(axis=1))])\n",
    "        hn=hn.view(inputs.shape[0], -1)\n",
    "        #use last hidden state as sequence representation\n",
    "     #   hn=hn.squeeze()\n",
    "        return self.out_layer(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "rnn_model=BookGenreClassifier(SimpleRNN(32, 10, vocab_size=vocab_size,\n",
    "                                        nonlinearity='tanh',\n",
    "                                        dropout_p=0.5,\n",
    "                                        w2v_init=True,\n",
    "                                        ), \n",
    "                             loss=nn.CrossEntropyLoss(),#weight=torch.tensor(label_weights, dtype=torch.float32),\n",
    "                             lr=1e-3,\n",
    "                             lr_dc=0.5,\n",
    "                             lr_dc_step=4,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=20,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "         #   LearningRateFinder(min_lr=1e-4, num_training_steps=1000)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240616_192615-ggibuo4k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k' target=\"_blank\">different-frog-47</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | SimpleRNN        | 1.2 M \n",
      "-------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.660     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|| 130/130 [00:05<00:00, 22.49it/s, v_num=uo4k, train_loss=0.897, val_loss=2.350, val_acc=25.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|| 130/130 [00:05<00:00, 22.48it/s, v_num=uo4k, train_loss=0.897, val_loss=2.350, val_acc=25.30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>scheduler_lr</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>scheduler_lr</td><td>0.00025</td></tr><tr><td>train_loss</td><td>0.89707</td></tr><tr><td>trainer/global_step</td><td>2599</td></tr><tr><td>val_acc</td><td>25.32189</td></tr><tr><td>val_loss</td><td>2.35457</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-frog-47</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240616_192615-ggibuo4k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(rnn_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>scheduler_lr</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>scheduler_lr</td><td>0.00025</td></tr><tr><td>train_loss</td><td>0.14059</td></tr><tr><td>trainer/global_step</td><td>2339</td></tr><tr><td>val_acc</td><td>22.103</td></tr><tr><td>val_loss</td><td>4.35297</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-frost-41</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/1tthcp5y' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/1tthcp5y</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240616_190118-1tthcp5y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0, nonlinearity='none', w2v_init=False, num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.name='SimpleLSTM'\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity=nonlinearity\n",
    "        self.w2v_init=w2v_init\n",
    "        self.num_layers=num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.bimult=1+bidirectional\n",
    "        # last token states 'end of string' and is repeated multiple time at the end of an input\n",
    "        # therefore set its embedding to 0 with padding_idx=-1\n",
    "        self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1) \n",
    "        if w2v_init:\n",
    "            w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "            emb_lst = []\n",
    "            for v in range(vocab_size):\n",
    "                emb_lst.append(w2vmodel.wv[str(v)])\n",
    "            emb_mat = np.array(emb_lst)\n",
    "            self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "\n",
    "        self.model=nn.LSTM(input_size=hid_dim, \n",
    "                           hidden_size=hid_dim, \n",
    "                           batch_first=True, \n",
    "                           dropout=dropout_p, \n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional)\n",
    "      #  self.out_layer=nn.Linear(self.bimult*num_layers*hid_dim, out_dim)\n",
    "        self.out_layer=nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "                                     nn.Linear(self.bimult*num_layers*hid_dim+hid_dim, hid_dim),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.Linear(hid_dim, out_dim)\n",
    "                                     )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, lengths=x\n",
    "        rnn_input=self.emb(inputs)\n",
    "\n",
    "        avg_emb = torch.stack([rnn_input[i, :lengths[i]].mean(dim=0) for i in range(lengths.shape[0])])\n",
    "\n",
    "        h0 = torch.randn(self.bimult*self.num_layers, rnn_input.shape[0], rnn_input.shape[-1], device=rnn_input.device)\n",
    "        c0 = torch.randn(self.bimult*self.num_layers, rnn_input.shape[0], rnn_input.shape[-1], device=rnn_input.device)\n",
    "\n",
    "        rnn_input=pack_padded_sequence(rnn_input,lengths.to('cpu').to(int), batch_first=True, enforce_sorted=False)\n",
    "        hstates, (hn, cn) = self.model(rnn_input, (h0, c0))\n",
    "        #return hstates\n",
    "       # print(h_states.shape)\n",
    "    #    hn[0]=torch.stack([hstates[e, int(i)-1] for e,i in enumerate(lengths)])\n",
    "        hn=hn.view(inputs.shape[0], -1)\n",
    "       # hn=hn.squeeze()\n",
    "        return self.out_layer(torch.concat([avg_emb, hn], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "lstm_model=BookGenreClassifier(SimpleLSTM(128, 10, vocab_size=vocab_size,\n",
    "                                       # nonlinearity='tanh',\n",
    "                                        dropout_p=0.5,\n",
    "                                        w2v_init=True,\n",
    "                                        num_layers=1,\n",
    "                                        ), \n",
    "                             loss=nn.CrossEntropyLoss(weight=torch.tensor(label_weights, dtype=torch.float32)),\n",
    "                             lr=1e-3,\n",
    "                             lr_dc=0.5,\n",
    "                             lr_dc_step=4,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=50,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "         #   LearningRateFinder(min_lr=1e-4, num_training_steps=1000)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2165: UserWarning: Run (8jy266yv) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240616_224804-9ml24rg0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/9ml24rg0' target=\"_blank\">winter-cosmos-70</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/9ml24rg0' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/9ml24rg0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | SimpleLSTM       | 4.8 M \n",
      "-------------------------------------------\n",
      "4.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 M     Total params\n",
      "19.268    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|| 130/130 [00:17<00:00,  7.43it/s, v_num=4rg0, train_loss=0.0985, val_loss=2.550, val_acc=40.60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>scheduler_lr</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>scheduler_lr</td><td>0.00013</td></tr><tr><td>train_loss</td><td>0.23102</td></tr><tr><td>trainer/global_step</td><td>2729</td></tr><tr><td>val_acc</td><td>40.55794</td></tr><tr><td>val_loss</td><td>2.54933</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-cosmos-70</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/9ml24rg0' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/9ml24rg0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240616_224804-9ml24rg0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lstm_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-jazz-60</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/8jy266yv' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/8jy266yv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240616_212924-8jy266yv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_stuff",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
