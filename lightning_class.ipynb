{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "from os import cpu_count\n",
    "from types import NoneType\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint, LearningRateFinder\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'genre', 'summary', 'input_ids', 'att_mask', 'label',\n",
       "       'mapped_inputs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_parquet('./data/books.par')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./data/tweets.csv', names=['index','genre','summary']).drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([149985, 835])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, AutoTokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token \n",
    "tokenized_summary=gpt2_tokenizer(df.summary.map(lambda x: x.strip().lower()).tolist(),\n",
    "                    return_tensors='pt',\n",
    "                    padding=True)\n",
    "tokenized_summary['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38690"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_summary['input_ids'][0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kacper/ecoNLP/lightning_class.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokens_dict\u001b[39m=\u001b[39m{}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tokenized_summary[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49munique()):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     tokens_dict[t\u001b[39m.\u001b[39mitem()]\u001b[39m=\u001b[39mi\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/_tensor.py:913\u001b[0m, in \u001b[0;36mTensor.unique\u001b[0;34m(self, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    904\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    905\u001b[0m         Tensor\u001b[39m.\u001b[39munique,\n\u001b[1;32m    906\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    911\u001b[0m         dim\u001b[39m=\u001b[39mdim,\n\u001b[1;32m    912\u001b[0m     )\n\u001b[0;32m--> 913\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49munique(\n\u001b[1;32m    914\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    915\u001b[0m     \u001b[39msorted\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39msorted\u001b[39;49m,\n\u001b[1;32m    916\u001b[0m     return_inverse\u001b[39m=\u001b[39;49mreturn_inverse,\n\u001b[1;32m    917\u001b[0m     return_counts\u001b[39m=\u001b[39;49mreturn_counts,\n\u001b[1;32m    918\u001b[0m     dim\u001b[39m=\u001b[39;49mdim,\n\u001b[1;32m    919\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/functional.py:991\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m    989\u001b[0m     \u001b[39mreturn\u001b[39;00m _unique_impl(\u001b[39minput\u001b[39m, \u001b[39msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 991\u001b[0m output, _, _ \u001b[39m=\u001b[39m _unique_impl(\u001b[39minput\u001b[39;49m, \u001b[39msorted\u001b[39;49m, return_inverse, return_counts, dim)\n\u001b[1;32m    992\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/functional.py:905\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    897\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39munique_dim(\n\u001b[1;32m    898\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m    899\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m         return_counts\u001b[39m=\u001b[39mreturn_counts,\n\u001b[1;32m    903\u001b[0m     )\n\u001b[1;32m    904\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 905\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_unique2(\n\u001b[1;32m    906\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    907\u001b[0m         \u001b[39msorted\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39msorted\u001b[39;49m,\n\u001b[1;32m    908\u001b[0m         return_inverse\u001b[39m=\u001b[39;49mreturn_inverse,\n\u001b[1;32m    909\u001b[0m         return_counts\u001b[39m=\u001b[39;49mreturn_counts,\n\u001b[1;32m    910\u001b[0m     )\n\u001b[1;32m    911\u001b[0m \u001b[39mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens_dict={}\n",
    "for i, t in enumerate(tokenized_summary['input_ids'].unique()):\n",
    "    tokens_dict[t.item()]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_summary['input_ids']=tokenized_summary['input_ids'].apply_(lambda x: tokens_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21301"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_summary['input_ids'][0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([149985])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_summary['attention_mask'].sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(data_utils.Dataset):\n",
    "    def __init__(self, tokenized_summary, labels, idxs):\n",
    "        self.input_ids=tokenized_summary['input_ids'][idxs]\n",
    "        self.lengths=tokenized_summary['attention_mask'].sum(axis=1)[idxs]\n",
    "        self.y=torch.tensor(labels, dtype=torch.uint8)[idxs]\n",
    "        self.no_classes=2\n",
    "        self.max_len=self.input_ids.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indexes):\n",
    "        return self.input_ids[indexes], self.lengths[indexes], self.y[indexes]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs=torch.rand(df.shape[0])>0.8\n",
    "val_idxs=~train_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=TweetDataset(tokenized_summary, df.genre.values, train_idxs)\n",
    "val_data=TweetDataset(tokenized_summary, df.genre.values, val_idxs)\n",
    "\n",
    "train_dataloader=data_utils.DataLoader(train_data, batch_size=32, num_workers=cpu_count(),\n",
    "                                       shuffle=True, drop_last=True)\n",
    "val_dataloader=data_utils.DataLoader(val_data, batch_size=32, num_workers=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(data_utils.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.input_ids=torch.tensor(df.mapped_inputs, dtype=torch.int32)\n",
    "        self.lengths=torch.tensor(df.att_mask.map(sum), dtype=torch.int32)\n",
    "      #  self.att_masks=torch.tensor(df.att_mask, dtype=torch.float32)\n",
    "        self.y=torch.tensor(df.label.values, dtype=torch.uint8)\n",
    "        self.no_classes=df.label.nunique()\n",
    "        self.max_len=self.input_ids.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indexes):\n",
    "       # batch_max_len= torch.max(self.lengths[indexes])\n",
    "        return self.input_ids[indexes], self.lengths[indexes], self.y[indexes]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSampler(data_utils.Sampler):\n",
    "    def __init__(self, dataset, batch_size, shuffle=False, drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.length\n",
    "\n",
    "    def __iter__(self):\n",
    "        order = np.arange(len(self))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(order)\n",
    "        if len(self) % self.batch_size:\n",
    "            for i in range(0, len(self) - self.batch_size, self.batch_size):\n",
    "                yield order[i : i + self.batch_size]\n",
    "            if not self.drop_last:\n",
    "                yield order[-(len(self) % self.batch_size) :]\n",
    "        else:\n",
    "            for i in range(0, len(self), self.batch_size):\n",
    "                yield order[i : i + self.batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedDataset(data_utils.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.input_ids=torch.tensor(df.mapped_inputs, dtype=torch.int32)\n",
    "        self.lengths=torch.tensor(df.att_mask.map(sum), dtype=torch.int32)\n",
    "        self.y=torch.tensor(df.label.values, dtype=torch.uint8)\n",
    "        self.no_classes=df.label.nunique()\n",
    "        self.max_len=self.input_ids.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indexes):\n",
    "       \n",
    "        return pack_padded_sequence(self.input_ids[indexes].view(-1, self.max_len), \n",
    "                                    self.lengths[indexes].view(-1), \n",
    "                                    batch_first=True, enforce_sorted=False), self.y[indexes]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_elementwise_apply(fn, packed_sequence):\n",
    "    \"\"\"applies a pointwise function fn to each element in packed_sequence\"\"\"\n",
    "    return torch.nn.utils.rnn.PackedSequence(fn(packed_sequence.data), packed_sequence.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pack_padded_sequence(emb(b[0]), b[1], batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_elementwise_apply(emb, val_data[:32][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[np.random.permutation(df.shape[0])].reset_index(drop=True)\n",
    "split=int(df.shape[0]*0.9)\n",
    "\n",
    "train_df=df.iloc[:split]\n",
    "val_df=df.iloc[split:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02315746, 0.03168706, 0.0408208 , 0.03396712, 0.03434735,\n",
       "       0.01992444, 0.20685595, 0.18596141, 0.20920658, 0.21407185])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,label_weights=np.unique(train_df.label, return_counts=True)\n",
    "label_weights=1/label_weights\n",
    "label_weights=label_weights/np.sum(label_weights)\n",
    "label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4282/4220535775.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  self.input_ids=torch.tensor(df.mapped_inputs, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "train_data=BookDataset(train_df)\n",
    "val_data=BookDataset(val_df)\n",
    "\n",
    "train_dataloader=data_utils.DataLoader(train_data, batch_size=32, num_workers=cpu_count(),\n",
    "                                       shuffle=True, drop_last=True)\n",
    "val_dataloader=data_utils.DataLoader(val_data, batch_size=32, num_workers=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pl module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookGenreClassifier(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-2, loss=nn.CrossEntropyLoss(), l2=1e-5, lr_dc_step=3, lr_dc=0.1, weight_init='normal', **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model','loss'])\n",
    "        if isinstance(loss.weight, NoneType):\n",
    "            weighted_loss=False\n",
    "        else: weighted_loss=True\n",
    "        self.save_hyperparameters({'name':model.name, \n",
    "                                   'hid_dim':model.hid_dim,\n",
    "                                   'dropout_p':model.dropout_p,\n",
    "                                   'w2v_init':model.w2v_init,\n",
    "                                   'num_layers':model.num_layers,\n",
    "                                   'nonlinearity':model.nonlinearity,\n",
    "                                   'bidirectional':model.bidirectional,\n",
    "                                   'weighted_loss':weighted_loss})\n",
    "        self.lr=lr\n",
    "        self.loss=loss\n",
    "        self.model=model\n",
    "        if weight_init!='normal':\n",
    "            self.reset_parameters(weight_init)\n",
    "\n",
    "    def reset_parameters(self, weight_init):\n",
    "        if weight_init == \"uniform\":\n",
    "            stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "            for weight in self.parameters():\n",
    "                nn.init.uniform_(weight, -stdv, stdv)\n",
    "        elif weight_init == \"normal\":\n",
    "            for weight in self.parameters():\n",
    "                nn.init.normal_(weight, 0, 0.1)\n",
    "        elif weight_init == \"xavier_normal\":\n",
    "            for weight in self.parameters():\n",
    "                if len(weight.shape) < 2:\n",
    "                    nn.init.normal_(weight, 0, 0.1)\n",
    "                else:\n",
    "                    nn.init.xavier_normal_(weight)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Weight initialization of type {weight_init} not implemented!\"\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[:-1]\n",
    "        y = batch[-1]\n",
    "        logits=self(x)\n",
    "        loss=self.loss(logits, y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, batch, mode=None):\n",
    "        x = batch[:-1]\n",
    "        y = batch[-1]\n",
    "        logits=self(x)\n",
    "\n",
    "        loss=self.loss(logits, y)\n",
    "\n",
    "        preds=torch.argmax(logits, axis=1)\n",
    "        acc=torch.sum(preds==y)/y.shape[0]\n",
    "        # TODO add more metrics\n",
    "\n",
    "        if mode:\n",
    "            self.log(mode+'_loss', loss,  prog_bar=True)\n",
    "            self.log(mode+'_acc', 100*acc,  prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        return self.evaluate(batch, \"val\")\n",
    "    def test_step(self, batch, *args, **kwargs):\n",
    "        return self.evaluate(batch, \"test\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.hparams.l2\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            patience=self.hparams.lr_dc_step,\n",
    "            factor=self.hparams.lr_dc,\n",
    "            cooldown=1,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_acc\",\n",
    "                \"strict\": False,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"name\": \"scheduler_lr\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module): \n",
    "    def __init__(self, in_dim=7031, hid_dim=128, out_dim=10):\n",
    "        # dummy model as an example, just one hidden layer straight from list of tokens\n",
    "        super().__init__()\n",
    "        self.name='DummyModel'\n",
    "        self.dropout_p=0\n",
    "        self.l1=nn.Linear(in_dim, hid_dim)\n",
    "        self.nonlinear=nn.Tanh()\n",
    "        self.l2=nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_ids, att_mask = x\n",
    "        x=in_ids*att_mask\n",
    "        x=self.l1(x)\n",
    "        x=self.nonlinear(x)\n",
    "        return self.l2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_model=BookGenreClassifier(DummyModel(), loss=nn.CrossEntropyLoss(weight=torch.tensor(label_weights, dtype=torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/szymon/pythonvenvs/rocmwork/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0      | train\n",
      "1 | model | DummyModel       | 901 K  | train\n",
      "---------------------------------------------------\n",
      "901 K     Trainable params\n",
      "0         Non-trainable params\n",
      "901 K     Total params\n",
      "3.606     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a7ad7fdae44ddbaf25d5bd4179b128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                   | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(dm_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3602,  6904,   173,  ..., 26305, 26305, 26305], dtype=torch.int32),\n",
       " tensor(1074, dtype=torch.int32),\n",
       " tensor(0, dtype=torch.uint8))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kacper/ecoNLP/lightning_class.ipynb Cell 38\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# for tweets\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vocab_size\u001b[39m=\u001b[39mtokenized_summary[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmax()\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bookwords \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#for s in df.summary:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kacper/ecoNLP/lightning_class.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#    bookwords.append(list(tokenize(s, lowercase=True)))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_summary' is not defined"
     ]
    }
   ],
   "source": [
    "# for tweets\n",
    "vocab_size=tokenized_summary['input_ids'].max()+1\n",
    "bookwords = []\n",
    "#for s in df.summary:\n",
    "#    bookwords.append(list(tokenize(s, lowercase=True)))\n",
    "for s, a in zip(tokenized_summary['input_ids'], tokenized_summary['attention_mask']):\n",
    "    bookwords.append([str(i.item()) for i in s[a.to(bool)]]+[str(s[-1].item())])\n",
    "print(bookwords[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26306"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_set=set()\n",
    "for tokens in df.mapped_inputs.values:\n",
    "    tokens_set=tokens_set.union(set(tokens))\n",
    "vocab_size=len(tokens_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3602', '6904', '173', '6462', '592', '170', '1346', '527', '175', '1048', '11', '10380', '2707', '16953', '211', '151', '10878', '175', '151', '1176', '336', '447', '5583', '179', '203', '7293', '13', '222', '7410', '467', '213', '146', '1304', '179', '3605', '840', '246', '981', '9959', '165', '395', '1', '723', '222', '7677', '293', '333', '14305', '342', '2403', '1013', '9877', '13', '562', '6008', '173', '2638', '479', '12', '47', '270', '42', '179', '8048', '151', '1508', '232', '151', '3083', '175', '151', '4496', '11', '530', '203', '1599', '204', '1006', '3495', '173', '13080', '151', '9959', '165', '395', '11', '146', '10491', '175', '170', '4753', '14805', '175', '24600', '205', '11', '4920', '209', '172', '151', '1503', '4180', '13', '4920', '209', '172', '221', '4531', '336', '447', '13778', '176', '151', '10491', '179', '4753', '2772', '264', '255', '527', '173', '1636', '173', '1247', '539', '13', '10417', '11', '1656', '151', '145', '1099', '276', '11', '204', '151', '20828', '4024', '175', '3738', '41', '2478', '1176', '11', '178', '446', '336', '4008', '211', '151', '218', '4317', '2889', '11', '377', '1256', '195', '34', '11', '336', '447', '3894', '5638', '13', '312', '565', '2259', '179', '178', '446', '656', '213', '151', '2422', '3978', '11', '179', '5377', '211', '222', '203', '825', '5170', '13', '312', '565', '2259', '9096', '151', '2422', '3978', '11', '173', '196', '3000', '151', '1100', '2422', '3978', '13', '178', '446', '1324', '16953', '211', '312', '565', '2259', '336', '447', '3724', '4753', '25392', '293', '146', '1712', '542', '2537', '17781', '11', '1592', '547', '236', '151', '3743', '13', '312', '565', '2259', '8214', '151', '444', '23960', '213', '151', '1918', '175', '2422', '3978', '11', '179', '5906', '634', '222', '812', '25392', '173', '4297', '1254', '513', '2381', '17097', '12', '53', '6159', '13', '151', '10762', '294', '146', '15008', '510', '211', '151', '2381', '17097', '12', '53', '6159', '203', '151', '18549', '175', '377', '4487', '11', '179', '388', '8913', '312', '565', '2259', '236', '151', '507', '2422', '3978', '13', '312', '565', '2259', '14788', '151', '24795', '11', '236', '725', '236', '170', '15198', '11', '12332', '15992', '11', '293', '151', '3442', '15198', '10762', '173', '863', '539', '1154', '151', '9992', '3707', '13', '351', '3402', '11', '15116', '11', '179', '12666', '151', '8387', '179', '10617', '175', '151', '9992', '3707', '4024', '175', '151', '145', '1099', '276', '11', '817', '351', '3249', '213', '151', '18638', '173', '10157', '1822', '151', '11987', '11', '388', '211', '151', '145', '1099', '276', '348', '1451', '4561', '13', '176', '151', '9992', '3707', '4216', '175', '151', '145', '1099', '276', '11', '213', '4266', '203', '6008', '293', '712', '734', '664', '175', '987', '11', '477', '222', '16953', '211', '151', '2586', '336', '447', '7135', '287', '170', '11112', '11', '179', '211', '151', '14805', '179', '1717', '344', '175', '333', '5400', '2403', '9050', '264', '2183', '13', '222', '493', '5478', '370', '211', '151', '17369', '175', '151', '3349', '203', '177', '3705', '191', '10181', '857', '13', '213', '4266', '8071', '177', '3705', '191', '10181', '857', '11', '179', '191', '10181', '857', '2768', '213', '4266', '173', '863', '2588', '333', '4261', '11', '388', '211', '191', '10181', '857', '329', '5616', '457', '5788', '25490', '213', '151', '12774', '13', '1809', '11', '213', '4266', '336', '146', '1670', '175', '16086', '293', '151', '637', '2183', '14805', '11', '388', '222', '11840', '173', '3737', '11', '407', '2885', '244', '191', '10181', '857', '13', '213', '4266', '4538', '1149', '11', '173', '5377', '146', '16173', '25732', '176', '333', '7420', '11', '3335', '222', '414', '3424', '3584', '146', '10823', '232', '13', '222', '16953', '211', '151', '16173', '18670', '151', '3274', '547', '634', '151', '3274', '250', '5242', '13364', '211', '6005', '338', '447', '18628', '11', '179', '151', '3862', '175', '151', '3274', '250', '146', '4322', '13556', '13', '176', '1214', '213', '25732', '151', '3274', '11', '151', '16173', '2925', '213', '4266', '146', '10491', '13', '562', '1893', '223', '1149', '11', '222', '5478', '370', '211', '333', '2964', '203', '7592', '11', '179', '211', '351', '931', '1057', '151', '10491', '221', '22281', '3906', '13', '222', '9355', '173', '294', '223', '3135', '513', '151', '1387', '282', '9302', '595', '151', '1176', '13', '10380', '2707', '16953', '211', '151', '10491', '250', '7269', '293', '146', '13556', '11', '179', '211', '223', '336', '3550', '376', '176', '151', '2343', '175', '1937', '22815', '213', '4266', '13', '530', '4885', '173', '6462', '151', '10491', '11', '342', '203', '9586', '179', '5932', '287', '213', '4266', '221', '7920', '13', '213', '4266', '2223', '467', '11', '342', '530', '11840', '173', '2691', '179', '222', '336', '467', '6628', '376', '176', '146', '7690', '13', '222', '14724', '211', '151', '218', '4317', '264', '7723', '173', '2684', '151', '145', '1099', '276', '11', '179', '1154', '333', '166', '1749', '13', '222', '3108', '213', '4718', '293', '191', '10181', '857', '11', '342', '191', '10181', '857', '2223', '333', '10861', '13', '191', '10181', '857', '10037', '173', '1333', '213', '4266', '547', '1873', '2827', '1222', '470', '222', '10037', '173', '191', '14332', '151', '4261', '13', '3326', '173', '575', '11', '213', '4266', '10037', '13', '1324', '11', '862', '630', '11', '146', '1304', '175', '213', '4266', '221', '179', '151', '1231', '8305', '175', '151', '8396', '22985', '10037', '173', '863', '213', '4266', '13', '222', '9355', '173', '3046', '151', '8396', '22985', '173', '863', '213', '4266', '11', '342', '351', '825', '329', '338', '1476', '151', '166', '1749', '13', '222', '642', '2223', '10380', '2707', '613', '11', '179', '10037', '173', '1333', '467', '151', '10491', '470', '530', '348', '2316', '2638', '479', '12', '47', '270', '42', '173', '3046', '4718', '13', '530', '9776', '2638', '479', '12', '47', '270', '42', '11', '342', '16953', '211', '151', '4753', '2772', '5207', '204', '13950', '151', '10491', '294', '9109', '151', '3083', '175', '151', '4496', '179', '344', '467', '518', '13', '213', '4266', '3920', '467', '530', '221', '1247', '173', '335', '11', '342', '530', '825', '2768', '173', '863', '539', '11', '388', '530', '2772', '4920', '209', '172', '173', '3046', '146', '15658', '5165', '484', '151', '23411', '2996', '23917', '13', '151', '23411', '2996', '23917', '203', '5026', '11', '179', '530', '10554', '173', '2236', '4920', '209', '172', '592', '513', '151', '10491', '13', '146', '1182', '11897', '175', '151', '218', '4317', '23917', '203', '825', '5170', '11', '236', '725', '236', '12332', '15992', '11', '151', '2422', '3978', '11', '179', '178', '446', '13', '151', '23917', '1348', '338', '1632', '726', '5065', '179', '264', '2876', '7179', '287', '213', '4266', '179', '333', '1222', '11', '1809', '11', '151', '15198', '19332', '13', '151', '2422', '3978', '3928', '592', '513', '146', '1431', '634', '151', '4753', '2772', '16373', '539', '11', '388', '222', '3468', '6520', '13', '222', '8214', '213', '4266', '176', '170', '1867', '173', '13987', '333', '5670', '11', '342', '213', '4266', '9096', '539', '13', '151', '2422', '3978', '221', '21091', '11', '178', '446', '11', '9355', '279', '173', '146', '12468', '333', '3925', '179', '2022', '4418', '213', '4266', '13', '213', '4266', '16953', '211', '862', '630', '250', '2415', '956', '151', '3469', '179', '211', '151', '218', '4317', '411', '1599', '236', '146', '20283', '388', '211', '151', '1176', '424', '196', '23234', '13', '213', '4266', '11', '10380', '2707', '11', '179', '178', '446', '1214', '173', '151', '1176', '547', '173', '5377', '211', '2636', '1898', '336', '447', '14938', '11', '151', '10491', '203', '3821', '179', '151', '547', '1282', '1150', '1808', '203', '146', '7603', '221', '3991', '13', '26305']]\n"
     ]
    }
   ],
   "source": [
    "bookwords = []\n",
    "#for s in df.summary:\n",
    "#    bookwords.append(list(tokenize(s, lowercase=True)))\n",
    "for s, a in zip(df.mapped_inputs, df.att_mask):\n",
    "    bookwords.append([str(i) for i in s[a.astype(bool)]]+[str(s[-1])])\n",
    "print(bookwords[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del train_df\n",
    "del val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSimple(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.name='Word2VecSimple'\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity='none'\n",
    "        self.w2v_init=True\n",
    "        self.num_layers=0\n",
    "        self.bidirectional=False\n",
    "        self.hid_dim=hid_dim\n",
    "\n",
    "        w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "        self.emb = nn.Embedding(vocab_size, hid_dim, padding_idx=-1)\n",
    "\n",
    "        emb_lst = []\n",
    "        for v in range(vocab_size):\n",
    "            emb_lst.append(w2vmodel.wv[str(v)])\n",
    "        \n",
    "        emb_mat = np.array(emb_lst)\n",
    "        self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "        # load embeddings from pretrained word2vec\n",
    "        #self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1)\n",
    "        self.out_layer=nn.Linear(hid_dim, out_dim)\n",
    "        #self.out_layer=nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "         #                            nn.Linear(hid_dim, hid_dim),\n",
    "          #                           nn.Tanh(),\n",
    "           #                          nn.Dropout(p=0.2),\n",
    "            #                         nn.Linear(hid_dim, out_dim)\n",
    "             #                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, lengths = x\n",
    "        batch_max_len=torch.max(lengths)\n",
    "        w2v_output=self.emb(inputs[:,:batch_max_len])\n",
    "        #avg_output = w2v_output.mean(dim=1)\n",
    "        avg_output = torch.stack([w2v_output[i, :lengths[i]].mean(dim=0) for i in range(lengths.shape[0])])\n",
    "       # print(f\"w2v_output:{w2v_output.shape}\")\n",
    "        #print(f\"avg_output:{avg_output.shape}\")\n",
    "        return self.out_layer(avg_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_simple_model = BookGenreClassifier(Word2VecSimple(64, 10, vocab_size=vocab_size), \n",
    "                             loss=nn.CrossEntropyLoss(),#weight=torch.tensor(label_weights, dtype=torch.float32)),\n",
    "                             lr_dc=0.1,\n",
    "                             lr_dc_step=4,\n",
    "                             weight_init='xavier_normal',\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=False\n",
    "    )\n",
    "\n",
    "trainer=pl.Trainer(max_epochs=50,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "            LearningRateFinder( num_training_steps=200)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkpuchalskixiv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240617_135134-v9zwolfa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/v9zwolfa' target=\"_blank\">sandy-cosmos-129</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/v9zwolfa' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/v9zwolfa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  98%|█████████▊| 196/200 [00:03<00:00, 58.28it/s]`Trainer.fit` stopped: `max_steps=200` reached.\n",
      "Finding best initial lr: 100%|██████████| 200/200 [00:03<00:00, 53.50it/s]\n",
      "Learning rate set to 0.10964781961431852\n",
      "Restoring states from the checkpoint path at /home/kacper/ecoNLP/.lr_find_75491b96-5ed3-4913-8034-cb5532c1e2e6.ckpt\n",
      "Restored all states from the checkpoint at /home/kacper/ecoNLP/.lr_find_75491b96-5ed3-4913-8034-cb5532c1e2e6.ckpt\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | Word2VecSimple   | 1.7 M \n",
      "-------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.737     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at /home/kacper/ecoNLP/.lr_find_75491b96-5ed3-4913-8034-cb5532c1e2e6.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/130 [00:00<?, ?it/s, v_num=olfa, train_loss=1.950]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:382: `ModelCheckpoint(monitor='val_acc')` could not find the monitored key in the returned metrics: ['scheduler_lr', 'train_loss', 'epoch', 'step']. HINT: Did you call `log('val_acc', value)` in the `LightningModule`?\n",
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:381: ReduceLROnPlateau conditioned on metric val_acc which is not available but strict is set to `False`. Skipping learning rate update.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 130/130 [00:02<00:00, 46.80it/s, v_num=olfa, train_loss=0.113, val_loss=1.020, val_acc=68.50] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>scheduler_lr</td><td>███████▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▆▄▃▃▂▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▄▇▇█▆▆▇▇▆▇▆▆▆▇</td></tr><tr><td>val_loss</td><td>█▂▁▁▃▃▃▃▃▃▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>scheduler_lr</td><td>0.0001</td></tr><tr><td>train_loss</td><td>0.10797</td></tr><tr><td>trainer/global_step</td><td>2009</td></tr><tr><td>val_acc</td><td>68.45493</td></tr><tr><td>val_loss</td><td>1.02262</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-cosmos-129</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/v9zwolfa' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/v9zwolfa</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240617_135134-v9zwolfa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(w2v_simple_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0, nonlinearity='tanh', w2v_init=False,\n",
    "                 num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.name='SimpleRNN'\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity=nonlinearity\n",
    "        self.w2v_init=w2v_init\n",
    "        self.num_layers=num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.bimult=1+bidirectional\n",
    "        # last token states 'end of string' and is repeated multiple time at the end of an input\n",
    "        # therefore set its embedding to 0 with padding_idx=-1\n",
    "        self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1) \n",
    "        if w2v_init:\n",
    "            w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "            emb_lst = []\n",
    "            for v in range(vocab_size):\n",
    "                emb_lst.append(w2vmodel.wv[str(v)])\n",
    "            emb_mat = np.array(emb_lst)\n",
    "            self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "            \n",
    "        self.model=nn.RNN(input_size=hid_dim, \n",
    "                          hidden_size=hid_dim, \n",
    "                          batch_first=True, \n",
    "                          dropout=dropout_p,\n",
    "                          bidirectional=True,\n",
    "                          nonlinearity=nonlinearity)\n",
    "        self.out_layer= nn.Linear(self.bimult*num_layers*hid_dim+hid_dim, out_dim),\n",
    "        #nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "         #                            nn.Linear(hid_dim, hid_dim),\n",
    "          #                           nn.LeakyReLU(),\n",
    "           #                          nn.Dropout(p=0.2),\n",
    "            #                         nn.Linear(hid_dim, out_dim)\n",
    "             #                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, lengths=x\n",
    "        batch_max_len=torch.max(lengths)\n",
    "        rnn_input=self.emb(inputs[:,:batch_max_len])\n",
    "\n",
    "     #   avg_emb = torch.stack([rnn_input[i, :lengths[i]].mean(dim=0) for i in range(lengths.shape[0])])\n",
    "\n",
    "        h0 = torch.randn(self.bimult*self.num_layers, rnn_input.shape[0], rnn_input.shape[-1], device=rnn_input.device)\n",
    "\n",
    "        rnn_input=pack_padded_sequence(rnn_input,lengths.to('cpu').to(int), batch_first=True, enforce_sorted=False)\n",
    "        hstates, hn = self.model(rnn_input, h0)\n",
    "        padded_hstates, lengths=[x.to(inputs.device) for x in pad_packed_sequence(hstates, batch_first=True)]\n",
    "        hstates_avg=padded_hstates.sum(dim=1).div(lengths.float().unsqueeze(dim=1))\n",
    "        #return hstates\n",
    "       # print(h_states.shape)\n",
    "    #    hn[0]=torch.stack([hstates[e, int(i)-1] for e,i in enumerate(lengths)])\n",
    "        hn=hn.view(inputs.shape[0], -1)\n",
    "       # hn=hn.squeeze()\n",
    "       #return self.out_layer(hn)\n",
    "        return self.out_layer(torch.concat([hstates_avg, hn], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "rnn_model=BookGenreClassifier(SimpleRNN(32, 10, vocab_size=vocab_size,\n",
    "                                        nonlinearity='tanh',\n",
    "                                        dropout_p=0.5,\n",
    "                                        w2v_init=True,\n",
    "                                        ), \n",
    "                             loss=nn.CrossEntropyLoss(),#weight=torch.tensor(label_weights, dtype=torch.float32),\n",
    "                             lr=1e-3,\n",
    "                             lr_dc=0.5,\n",
    "                             lr_dc_step=4,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=20,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "         #   LearningRateFinder(min_lr=1e-4, num_training_steps=1000)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240616_192615-ggibuo4k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k' target=\"_blank\">different-frog-47</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | SimpleRNN        | 1.2 M \n",
      "-------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.660     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 130/130 [00:05<00:00, 22.49it/s, v_num=uo4k, train_loss=0.897, val_loss=2.350, val_acc=25.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 130/130 [00:05<00:00, 22.48it/s, v_num=uo4k, train_loss=0.897, val_loss=2.350, val_acc=25.30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>scheduler_lr</td><td>████████████▃▃▃▃▃▃▁▁</td></tr><tr><td>train_loss</td><td>█▇▇█▇█▇▇▇▇▆▇▆▆▆▇▆▆▅▅▃▅▄▄▄▃▃▃▃▄▄▂▃▂▁▁▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▃▄▁▃▄▅▅▆▆▄▆▅▆█▇▆▄▆▄</td></tr><tr><td>val_loss</td><td>▂▁▁▁▁▁▁▁▁▂▃▃▄▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>scheduler_lr</td><td>0.00025</td></tr><tr><td>train_loss</td><td>0.89707</td></tr><tr><td>trainer/global_step</td><td>2599</td></tr><tr><td>val_acc</td><td>25.32189</td></tr><tr><td>val_loss</td><td>2.35457</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-frog-47</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/ggibuo4k</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240616_192615-ggibuo4k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(rnn_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▆▆▆▁▁▂▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇███</td></tr><tr><td>scheduler_lr</td><td>███████▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▇███▇▇▆▆▆▆▆▅▅▄▅█▇███▇▆▅▇▄▄▃▄▃▂▂▂▂▂▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▆▆▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>val_acc</td><td>▂▄▄▂▃▃▁▂▃▄▃▄▃▂▆▄▄▆▇▆█▅▆▆▄▅▅▄▄▃▃</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▂▂▂▃▃▃▄▁▁▁▁▁▁▁▂▃▄▄▅▆▆▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>scheduler_lr</td><td>0.00025</td></tr><tr><td>train_loss</td><td>0.14059</td></tr><tr><td>trainer/global_step</td><td>2339</td></tr><tr><td>val_acc</td><td>22.103</td></tr><tr><td>val_loss</td><td>4.35297</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-frost-41</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/1tthcp5y' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/1tthcp5y</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240616_190118-1tthcp5y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0, nonlinearity='none', w2v_init=False, num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.name='SimpleLSTM'\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity=nonlinearity\n",
    "        self.w2v_init=w2v_init\n",
    "        self.num_layers=num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.bimult=1+bidirectional\n",
    "        self.hid_dim=hid_dim\n",
    "        # last token states 'end of string' and is repeated multiple time at the end of an input\n",
    "        # therefore set its embedding to 0 with padding_idx=-1\n",
    "        self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1) \n",
    "        if w2v_init:\n",
    "            w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "            emb_lst = []\n",
    "            for v in range(vocab_size):\n",
    "                emb_lst.append(w2vmodel.wv[str(v)])\n",
    "            emb_mat = np.array(emb_lst)\n",
    "            self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "\n",
    "        self.model=nn.LSTM(input_size=hid_dim, \n",
    "                           hidden_size=hid_dim, \n",
    "                           batch_first=True, \n",
    "                           dropout=dropout_p, \n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional)\n",
    "      #  self.out_layer=nn.Linear(self.bimult*num_layers*hid_dim, out_dim)\n",
    "        self.out_layer=nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "                                     nn.Linear(self.bimult*num_layers*hid_dim+hid_dim, hid_dim),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.Linear(hid_dim, out_dim)\n",
    "                                     )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs, lengths=x\n",
    "        batch_max_len=torch.max(lengths)\n",
    "        rnn_input=self.emb(inputs[:,:batch_max_len])\n",
    "        rnn_input=rnn_input.div(rnn_input.sum(axis=2).view(inputs.shape[0],batch_max_len,1))\n",
    "\n",
    "     #   avg_emb = torch.stack([rnn_input[i, :lengths[i]].mean(dim=0) for i in range(lengths.shape[0])])\n",
    "\n",
    "        h0 = torch.randn(self.bimult*self.num_layers, rnn_input.shape[0], rnn_input.shape[-1], device=rnn_input.device)\n",
    "        c0 = torch.randn(self.bimult*self.num_layers, rnn_input.shape[0], rnn_input.shape[-1], device=rnn_input.device)\n",
    "\n",
    "        rnn_input=pack_padded_sequence(rnn_input,lengths.to('cpu').to(int), batch_first=True, enforce_sorted=False)\n",
    "        hstates, (hn, _) = self.model(rnn_input, (h0, c0))\n",
    "        padded_hstates, lengths=[x.to(inputs.device) for x in pad_packed_sequence(hstates, batch_first=True)]\n",
    "        hstates_avg=padded_hstates.sum(dim=1).div(lengths.float().unsqueeze(dim=1))\n",
    "        #return hstates\n",
    "       # print(h_states.shape)\n",
    "    #    hn[0]=torch.stack([hstates[e, int(i)-1] for e,i in enumerate(lengths)])\n",
    "        hn=hn.view(inputs.shape[0], -1)\n",
    "       # hn=hn.squeeze()\n",
    "       #return self.out_layer(hn)\n",
    "        return self.out_layer(torch.concat([hstates_avg, hn], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "lstm_model=BookGenreClassifier(SimpleLSTM(64, 10, vocab_size=vocab_size,\n",
    "                                       # nonlinearity='tanh',\n",
    "                                        dropout_p=0.5,\n",
    "                                        w2v_init=True,\n",
    "                                        num_layers=1,\n",
    "\n",
    "                                        ), \n",
    "                             loss=nn.CrossEntropyLoss(),#weight=torch.tensor(label_weights, dtype=torch.float32)),\n",
    "                             lr=1e-4,\n",
    "                             lr_dc=0.25,\n",
    "                             lr_dc_step=4,\n",
    "                             weight_init='normal',\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=50,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "            LearningRateFinder(num_training_steps=200)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkpuchalskixiv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240617_215056-h7n70x61</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/h7n70x61' target=\"_blank\">true-cloud-139</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/h7n70x61' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/h7n70x61</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr: 100%|██████████| 200/200 [00:12<00:00, 16.18it/s]\n",
      "`Trainer.fit` stopped: `max_steps=200` reached.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/kacper/ecoNLP/.lr_find_c09484b6-0937-4d8d-a898-63dc462fbe48.ckpt\n",
      "Restored all states from the checkpoint at /home/kacper/ecoNLP/.lr_find_c09484b6-0937-4d8d-a898-63dc462fbe48.ckpt\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | SimpleLSTM       | 1.7 M \n",
      "-------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.903     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at /home/kacper/ecoNLP/.lr_find_c09484b6-0937-4d8d-a898-63dc462fbe48.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/130 [00:00<?, ?it/s, v_num=0x61, train_loss=2.260]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:382: `ModelCheckpoint(monitor='val_acc')` could not find the monitored key in the returned metrics: ['scheduler_lr', 'train_loss', 'epoch', 'step']. HINT: Did you call `log('val_acc', value)` in the `LightningModule`?\n",
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:381: ReduceLROnPlateau conditioned on metric val_acc which is not available but strict is set to `False`. Skipping learning rate update.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 130/130 [00:08<00:00, 15.24it/s, v_num=0x61, train_loss=1.890, val_loss=1.870, val_acc=23.40]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>scheduler_lr</td><td>███████████▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▅▅▂▃▇▄▃▅▂▃▅▅▄▃▃▂▃▃▄▄▁▄▁▃▄▂▂▁▁▁▃▄▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_acc</td><td>▂▂▂█▁▃▅▄▅▅▄▅▃▃</td></tr><tr><td>val_loss</td><td>█▅▄▃█▇▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>14</td></tr><tr><td>scheduler_lr</td><td>3e-05</td></tr><tr><td>train_loss</td><td>1.82659</td></tr><tr><td>trainer/global_step</td><td>1879</td></tr><tr><td>val_acc</td><td>23.39056</td></tr><tr><td>val_loss</td><td>1.87253</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-cloud-139</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/h7n70x61' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/h7n70x61</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240617_215056-h7n70x61/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lstm_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim, vocab_size, dropout_p=0, nonlinearity=nn.ReLU(), w2v_init=False, num_layers=1, bidirectional=False,\n",
    "                 nheads=8,):\n",
    "        super().__init__()\n",
    "        self.name='SimpleTransformer'\n",
    "        self.hid_dim=hid_dim\n",
    "        self.dropout_p=dropout_p\n",
    "        self.nonlinearity=str(nonlinearity)\n",
    "        self.w2v_init=w2v_init\n",
    "        self.num_layers=num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.nheads=nheads\n",
    "        # last token states 'end of string' and is repeated multiple time at the end of an input\n",
    "        # therefore set its embedding to 0 with padding_idx=-1\n",
    "        self.emb=nn.Embedding(vocab_size, hid_dim, padding_idx=-1) \n",
    "        if w2v_init:\n",
    "            w2vmodel = Word2Vec(bookwords, vector_size=hid_dim, min_count=0)\n",
    "            emb_lst = []\n",
    "            for v in range(vocab_size):\n",
    "                emb_lst.append(w2vmodel.wv[str(v)])\n",
    "            emb_mat = np.array(emb_lst)\n",
    "            self.emb.load_state_dict({'weight': torch.from_numpy(emb_mat)})\n",
    "\n",
    "        t_layer= nn.TransformerEncoderLayer(d_model=hid_dim, nhead=nheads, batch_first=True, dropout=dropout_p, activation=nonlinearity)\n",
    "        self.model= nn.TransformerEncoder(t_layer, num_layers=num_layers)\n",
    "      #  self.out_layer=nn.Linear(self.bimult*num_layers*hid_dim, out_dim)\n",
    "        self.out_layer=nn.Sequential(nn.Dropout(p=dropout_p),\n",
    "                                     nn.Linear(hid_dim+hid_dim, hid_dim),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.Linear(hid_dim, out_dim)\n",
    "                                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inputs, lengths=x\n",
    "        max_len=min(512, torch.max(lengths))\n",
    "        t_input=self.emb(inputs[:, :max_len])\n",
    "\n",
    "        hstates=self.model(t_input)\n",
    "        hstates_avg=hstates.sum(dim=1).div(lengths.float().unsqueeze(dim=1))\n",
    "        hn=hstates[:,-1]\n",
    "\n",
    "        return self.out_layer(torch.concat([hstates_avg, hn], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model=BookGenreClassifier(TransformerModel(256, 10, vocab_size=vocab_size,\n",
    "                                       # nonlinearity='tanh',\n",
    "                                        dropout_p=0.5,\n",
    "                                        w2v_init=True,\n",
    "                                        num_layers=2,\n",
    "                                      #  nonlinearity=nn.Tanh()\n",
    "                                        ), \n",
    "                             loss=nn.CrossEntropyLoss(),#weight=torch.tensor(label_weights, dtype=torch.float32)),\n",
    "                             lr=1e-4,\n",
    "                             lr_dc=0.25,\n",
    "                             lr_dc_step=4,\n",
    "                             weight_init='normal'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "        project=\"ecoNLP\", entity=\"kpuchalskixiv\", log_model=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(max_epochs=50,\n",
    "                   callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_acc\", patience=10, mode=\"max\", check_finite=True, check_on_train_epoch_end=False\n",
    "            ),\n",
    "            LearningRateMonitor(),\n",
    "            ModelCheckpoint(monitor=\"val_acc\", mode=\"max\"),\n",
    "          #  LearningRateFinder(num_training_steps=200)\n",
    "            ],\n",
    "            logger=wandb_logger,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240617_140239-opumcj56</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/opumcj56' target=\"_blank\">divine-vortex-133</a></strong> to <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/opumcj56' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/opumcj56</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | loss  | CrossEntropyLoss | 0     \n",
      "1 | model | TransformerModel | 9.5 M \n",
      "-------------------------------------------\n",
      "9.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.5 M     Total params\n",
      "37.994    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 130/130 [00:14<00:00,  8.91it/s, v_num=cj56, train_loss=0.787, val_loss=1.780, val_acc=46.80]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>scheduler_lr</td><td>██████▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▆▆▇▆▅▆▆▄▅▄▅▄▄▄▄▂▂▃▃▂▂▂▃▃▃▂▃▁▂▃▃▃▂▂▂▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▂▂▃▄▆▇▇█▇▇▆██▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>█▇▆▆▄▃▂▁▁▂▂▅▃▃▄▅▅▅▆▅▅▆▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>23</td></tr><tr><td>scheduler_lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.99748</td></tr><tr><td>trainer/global_step</td><td>3119</td></tr><tr><td>val_acc</td><td>46.78112</td></tr><tr><td>val_loss</td><td>1.78359</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-vortex-133</strong> at: <a href='https://wandb.ai/kpuchalskixiv/ecoNLP/runs/opumcj56' target=\"_blank\">https://wandb.ai/kpuchalskixiv/ecoNLP/runs/opumcj56</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240617_140239-opumcj56/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(trans_model, train_dataloader, val_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asses cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run = api.run(\"kpuchalskixiv/ecoNLP/9f62gf68\")\n",
    "system_metrics = run.history(stream=\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system.network.sent</th>\n",
       "      <th>system.gpu.0.powerPercent</th>\n",
       "      <th>system.network.recv</th>\n",
       "      <th>system.cpu.1.cpu_percent</th>\n",
       "      <th>system.gpu.0.powerWatts</th>\n",
       "      <th>_wandb</th>\n",
       "      <th>system.disk.out</th>\n",
       "      <th>system.disk.\\.usageGB</th>\n",
       "      <th>system.gpu.0.temp</th>\n",
       "      <th>system.gpu.0.memory</th>\n",
       "      <th>...</th>\n",
       "      <th>system.proc.memory.availableMB</th>\n",
       "      <th>system.cpu</th>\n",
       "      <th>system.proc.cpu.threads</th>\n",
       "      <th>system.memory</th>\n",
       "      <th>system.cpu.0.cpu_percent</th>\n",
       "      <th>system.proc.memory.percent</th>\n",
       "      <th>system.disk.\\.usagePercent</th>\n",
       "      <th>system.disk.in</th>\n",
       "      <th>system.gpu.0.memoryAllocated</th>\n",
       "      <th>_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69511.07</td>\n",
       "      <td>32.63</td>\n",
       "      <td>53265.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>80.51</td>\n",
       "      <td>True</td>\n",
       "      <td>0.76</td>\n",
       "      <td>5689.88</td>\n",
       "      <td>41.27</td>\n",
       "      <td>15.27</td>\n",
       "      <td>...</td>\n",
       "      <td>28804.39</td>\n",
       "      <td>10.85</td>\n",
       "      <td>33</td>\n",
       "      <td>10.27</td>\n",
       "      <td>30.71</td>\n",
       "      <td>8.23</td>\n",
       "      <td>70.6</td>\n",
       "      <td>3.61</td>\n",
       "      <td>16.93</td>\n",
       "      <td>1.718619e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>531024.27</td>\n",
       "      <td>76.74</td>\n",
       "      <td>351139.33</td>\n",
       "      <td>30.50</td>\n",
       "      <td>192.76</td>\n",
       "      <td>True</td>\n",
       "      <td>1.86</td>\n",
       "      <td>5689.88</td>\n",
       "      <td>55.53</td>\n",
       "      <td>37.40</td>\n",
       "      <td>...</td>\n",
       "      <td>28892.33</td>\n",
       "      <td>25.05</td>\n",
       "      <td>33</td>\n",
       "      <td>10.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>8.23</td>\n",
       "      <td>70.6</td>\n",
       "      <td>6.68</td>\n",
       "      <td>19.75</td>\n",
       "      <td>1.718619e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1070869.73</td>\n",
       "      <td>77.82</td>\n",
       "      <td>687942.73</td>\n",
       "      <td>60.65</td>\n",
       "      <td>195.28</td>\n",
       "      <td>True</td>\n",
       "      <td>2.60</td>\n",
       "      <td>5689.88</td>\n",
       "      <td>64.60</td>\n",
       "      <td>37.87</td>\n",
       "      <td>...</td>\n",
       "      <td>28891.84</td>\n",
       "      <td>25.05</td>\n",
       "      <td>33</td>\n",
       "      <td>10.00</td>\n",
       "      <td>24.99</td>\n",
       "      <td>8.23</td>\n",
       "      <td>70.6</td>\n",
       "      <td>6.68</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1.718619e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1644629.47</td>\n",
       "      <td>77.47</td>\n",
       "      <td>1044225.80</td>\n",
       "      <td>77.00</td>\n",
       "      <td>195.82</td>\n",
       "      <td>True</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5689.88</td>\n",
       "      <td>72.07</td>\n",
       "      <td>37.93</td>\n",
       "      <td>...</td>\n",
       "      <td>28882.20</td>\n",
       "      <td>25.06</td>\n",
       "      <td>33</td>\n",
       "      <td>10.06</td>\n",
       "      <td>3.35</td>\n",
       "      <td>8.23</td>\n",
       "      <td>70.6</td>\n",
       "      <td>6.68</td>\n",
       "      <td>19.20</td>\n",
       "      <td>1.718619e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2251116.73</td>\n",
       "      <td>79.24</td>\n",
       "      <td>1421881.67</td>\n",
       "      <td>50.45</td>\n",
       "      <td>200.60</td>\n",
       "      <td>True</td>\n",
       "      <td>3.53</td>\n",
       "      <td>5689.88</td>\n",
       "      <td>74.53</td>\n",
       "      <td>37.20</td>\n",
       "      <td>...</td>\n",
       "      <td>28869.29</td>\n",
       "      <td>25.05</td>\n",
       "      <td>33</td>\n",
       "      <td>10.10</td>\n",
       "      <td>39.68</td>\n",
       "      <td>8.23</td>\n",
       "      <td>70.6</td>\n",
       "      <td>6.68</td>\n",
       "      <td>19.12</td>\n",
       "      <td>1.718619e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2902029.93</td>\n",
       "      <td>79.02</td>\n",
       "      <td>1824326.27</td>\n",
       "      <td>7.76</td>\n",
       "      <td>197.68</td>\n",
       "      <td>True</td>\n",
       "      <td>4.01</td>\n",
       "      <td>5689.88</td>\n",
       "      <td>74.00</td>\n",
       "      <td>37.53</td>\n",
       "      <td>...</td>\n",
       "      <td>28869.31</td>\n",
       "      <td>25.06</td>\n",
       "      <td>33</td>\n",
       "      <td>10.10</td>\n",
       "      <td>4.79</td>\n",
       "      <td>8.23</td>\n",
       "      <td>70.6</td>\n",
       "      <td>6.68</td>\n",
       "      <td>19.12</td>\n",
       "      <td>1.718619e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4045751.00</td>\n",
       "      <td>82.02</td>\n",
       "      <td>2531673.80</td>\n",
       "      <td>22.70</td>\n",
       "      <td>205.82</td>\n",
       "      <td>True</td>\n",
       "      <td>600.41</td>\n",
       "      <td>5689.88</td>\n",
       "      <td>73.67</td>\n",
       "      <td>37.67</td>\n",
       "      <td>...</td>\n",
       "      <td>28899.50</td>\n",
       "      <td>24.69</td>\n",
       "      <td>33</td>\n",
       "      <td>10.00</td>\n",
       "      <td>24.78</td>\n",
       "      <td>8.32</td>\n",
       "      <td>70.6</td>\n",
       "      <td>6.68</td>\n",
       "      <td>19.84</td>\n",
       "      <td>1.718619e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4371209.00</td>\n",
       "      <td>91.24</td>\n",
       "      <td>2796206.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>228.09</td>\n",
       "      <td>True</td>\n",
       "      <td>963.45</td>\n",
       "      <td>5689.88</td>\n",
       "      <td>72.20</td>\n",
       "      <td>31.00</td>\n",
       "      <td>...</td>\n",
       "      <td>28871.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>30</td>\n",
       "      <td>10.10</td>\n",
       "      <td>2.30</td>\n",
       "      <td>8.23</td>\n",
       "      <td>70.6</td>\n",
       "      <td>6.68</td>\n",
       "      <td>20.73</td>\n",
       "      <td>1.718620e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   system.network.sent  system.gpu.0.powerPercent  system.network.recv  \\\n",
       "0             69511.07                      32.63             53265.00   \n",
       "1            531024.27                      76.74            351139.33   \n",
       "2           1070869.73                      77.82            687942.73   \n",
       "3           1644629.47                      77.47           1044225.80   \n",
       "4           2251116.73                      79.24           1421881.67   \n",
       "5           2902029.93                      79.02           1824326.27   \n",
       "6           4045751.00                      82.02           2531673.80   \n",
       "7           4371209.00                      91.24           2796206.00   \n",
       "\n",
       "   system.cpu.1.cpu_percent  system.gpu.0.powerWatts  _wandb  system.disk.out  \\\n",
       "0                      1.99                    80.51    True             0.76   \n",
       "1                     30.50                   192.76    True             1.86   \n",
       "2                     60.65                   195.28    True             2.60   \n",
       "3                     77.00                   195.82    True             3.10   \n",
       "4                     50.45                   200.60    True             3.53   \n",
       "5                      7.76                   197.68    True             4.01   \n",
       "6                     22.70                   205.82    True           600.41   \n",
       "7                      1.20                   228.09    True           963.45   \n",
       "\n",
       "   system.disk.\\.usageGB  system.gpu.0.temp  system.gpu.0.memory  ...  \\\n",
       "0                5689.88              41.27                15.27  ...   \n",
       "1                5689.88              55.53                37.40  ...   \n",
       "2                5689.88              64.60                37.87  ...   \n",
       "3                5689.88              72.07                37.93  ...   \n",
       "4                5689.88              74.53                37.20  ...   \n",
       "5                5689.88              74.00                37.53  ...   \n",
       "6                5689.88              73.67                37.67  ...   \n",
       "7                5689.88              72.20                31.00  ...   \n",
       "\n",
       "   system.proc.memory.availableMB  system.cpu  system.proc.cpu.threads  \\\n",
       "0                        28804.39       10.85                       33   \n",
       "1                        28892.33       25.05                       33   \n",
       "2                        28891.84       25.05                       33   \n",
       "3                        28882.20       25.06                       33   \n",
       "4                        28869.29       25.05                       33   \n",
       "5                        28869.31       25.06                       33   \n",
       "6                        28899.50       24.69                       33   \n",
       "7                        28871.50        0.17                       30   \n",
       "\n",
       "   system.memory  system.cpu.0.cpu_percent  system.proc.memory.percent  \\\n",
       "0          10.27                     30.71                        8.23   \n",
       "1          10.00                     32.00                        8.23   \n",
       "2          10.00                     24.99                        8.23   \n",
       "3          10.06                      3.35                        8.23   \n",
       "4          10.10                     39.68                        8.23   \n",
       "5          10.10                      4.79                        8.23   \n",
       "6          10.00                     24.78                        8.32   \n",
       "7          10.10                      2.30                        8.23   \n",
       "\n",
       "   system.disk.\\.usagePercent  system.disk.in  system.gpu.0.memoryAllocated  \\\n",
       "0                        70.6            3.61                         16.93   \n",
       "1                        70.6            6.68                         19.75   \n",
       "2                        70.6            6.68                         19.59   \n",
       "3                        70.6            6.68                         19.20   \n",
       "4                        70.6            6.68                         19.12   \n",
       "5                        70.6            6.68                         19.12   \n",
       "6                        70.6            6.68                         19.84   \n",
       "7                        70.6            6.68                         20.73   \n",
       "\n",
       "     _timestamp  \n",
       "0  1.718619e+09  \n",
       "1  1.718619e+09  \n",
       "2  1.718619e+09  \n",
       "3  1.718619e+09  \n",
       "4  1.718619e+09  \n",
       "5  1.718619e+09  \n",
       "6  1.718619e+09  \n",
       "7  1.718620e+09  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.553547222222225"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_metrics['system.gpu.0.powerWatts'].mean()*(511/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030536579305555554"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.15*system_metrics['system.gpu.0.powerWatts'].mean()*(511/3600)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata=BookDataset(df)\n",
    "\n",
    "whole_datalaoder=data_utils.DataLoader(alldata, batch_size=32, num_workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kacper/anaconda3/envs/gpu_torch/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "test_trainer=pl.Trainer(max_epochs=1,\n",
    "                \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_simple_model.model.hid_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 146/146 [00:00<00:00, 236.01it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             95.12561798095703\n",
      "        test_loss           0.20292286574840546\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "test_trainer.test(w2v_simple_model, whole_datalaoder)\n",
    "infertime=(time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5918711148696778"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(75*infertime*100000/df.shape[0])/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006806517821001295"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((75*infertime*100000/df.shape[0])/3600)*1.15/1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_stuff",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
